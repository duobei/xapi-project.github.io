var relearn_search_index=[{content:`The Toolstack runs in an environment on a server (host) that has:
Physical hardware. The Xen hypervisor. The control domain (domain 0): the priviledged domain that the Toolstack runs in. Other, mostly unpriviledged domains, usually for guests (VMs). The Toolstack relies on various bits of software inside the control domain, and directly communicates with most of these:
Linux kernel including drivers for hardware and Xen paravirtualised devices (e.g. netback and blkback). Interacts through /sys and /proc, udev scripts, xenstore, … CentOS distibution including userspace tools and libraries. systemd, networking tools, … Xen-specific libraries, especially libxenctrl (a.k.a. libxc) xenstored: a key-value pair configuration database Accessible from all domains on a host, which makes it useful for inter-domain communication. The control domain has access to the entire xenstore database, while other domains only see sub-trees that are specific to that domain. Used for connecting VM disks and network interfaces, and other VM configuration options. Used for VM status reporting, e.g. the capabilities of the PV drivers (if installed), the IP address, etc. SM: Storage Manager plugins which connect xapi’s internal storage interfaces to the control APIs of external storage systems. stunnel: a daemon which decodes TLS and forwards traffic to xapi (and the other way around). Open vSwitch (OVS): a virtual network switch, used to connect VMs to network interfaces. The OVS offers several networking features that xapi takes advantage of. QEMU: emulation of various bits of hardware DEMU: emulation of Nvidia vGPUs xenguest emu-manager pvsproxy xenconsoled: allows access to guest consoles. This is common to all Xen hosts. The Toolstack also interacts with software that runs inside the guests:
PV drivers The guest agent `,description:"",tags:null,title:"Environment",uri:"/new-docs/toolstack/high-level/environment/index.html"},{content:`The XAPI Toolstack forms the main control plane of a pool of XenServer hosts. It allow the administrator to:
Configure the hardware resources of XenServer hosts: storage, networking, graphics, memory. Create, configure and destroy VMs and their virtual resources. Control the lifecycle of VMs. Monitor the status of hosts, VMs and related resources. To this, the Toolstack:
Exposes an API that can be accessed by external clients over HTTP(s). Exposes a CLI. Ensures that physical resources are configured when needed, and VMs receive the resources they require. Implements various features to help the administrator manage their systems. Monitors running VMs. Records metrics about physical and virtual resources. `,description:"",tags:null,title:"Responsibilities",uri:"/new-docs/toolstack/responsibilities/index.html"},{content:" Responsibilities High-level architecture Environment Daemons Interfaces Features Disaster Recovery High-Availability Snapshots vGPU Xapi Storage Migration ",description:"",tags:null,title:"The XAPI Toolstack",uri:"/new-docs/toolstack/index.html"},{content:`The Toolstack consists of a set of co-operating daemons:
xapi manages clusters of hosts, co-ordinating access to shared storage and networking. xenopsd a low-level “domain manager” which takes care of creating, suspending, resuming, migrating, rebooting domains by interacting with Xen via libxc and libxl. xcp-rrdd a performance counter monitoring daemon which aggregates “datasources” defined via a plugin API and records history for each. There are various rrdd-plugin daemons: xcp-rrdd-gpumon xcp-rrdd-iostat xcp-rrdd-squeezed xcp-rrdd-xenpm xcp-networkd a host network manager which takes care of configuring interfaces, bridges and OpenVSwitch instances squeezed a daemon in charge of VM memory management xapi-storage-script for storage manipulation over SMAPIv3 message-switch exchanges messages between the daemons on a host v6d controls which features are enabled. forkexecd a helper daemon that assists the above daemons with executing binaries and scripts xhad The High-Availability daemon perfmon a daemon which monitors performance counters and sends “alerts” if values exceed some pre-defined threshold mpathalert a daemon which monitors “storage paths” and sends “alerts” if paths fail and need repair wsproxy handles access to VM consoles `,description:"",tags:null,title:"Daemons",uri:"/new-docs/toolstack/high-level/daemons/index.html"},{content:`Xapi is the xapi-project host and cluster manager.
Xapi is responsible for:
providing a stable interface (the XenAPI) allowing one client to manage multiple hosts hosting the “xe” CLI authenticating users and applying role-based access control locking resources (in particular disks) allowing storage to be managed through plugins planning and coping with host failures (“High Availability”) storing VM and host configuration generating alerts managing software patching Principles The XenAPI interface must remain backwards compatible, allowing older clients to continue working Xapi delegates all Xenstore/libxc/libxl access to Xenopsd, so Xapi could be run in an unprivileged helper domain Xapi delegates the low-level storage manipulation to SM plugins. Xapi delegates setting up host networking to xcp-networkd. Xapi delegates monitoring performance counters to xcp-rrdd. Overview The following diagram shows the internals of Xapi:
The top of the diagram shows the XenAPI clients: XenCenter, XenOrchestra, OpenStack and CloudStack using XenAPI and HTTP GET/PUT over ports 80 and 443 to talk to xapi. These XenAPI (JSON-RPC or XML-RPC over HTTP POST) and HTTP GET/PUT are always authenticated using either PAM (by default using the local passwd and group files) or through Active Directory.
The APIs are classified into categories:
coordinator-only: these are the majority of current APIs. The coordinator should be called and relied upon to forward the call to the right place with the right locks held. normally-local: these are performance special cases such as disk import/export and console connection which are sent directly to hosts which have the most efficient access to the data. emergency: these deal with scenarios where the coordinator is offline If the incoming API call should be resent to the coordinator than a XenAPI HOST_IS_SLAVE error message containing the coordinator’s IP is sent to the client.
Once past the initial checks, API calls enter the “message forwarding” layer which
locks resources (via the current_operations mechanism) decides which host should execute the request. If the request should run locally then a direct function call is used; otherwise the message forwarding code makes a synchronous API call to a specific other host. Note: Xapi currently employs a “thread per request” model which causes one full POSIX thread to be created for every request. Even when a request is forwarded the full thread persists, blocking for the result to become available.
If the XenAPI call is a VM lifecycle operation then it is converted into a Xenopsd API call and forwarded over a Unix domain socket. Xapi and Xenopsd have similar notions of cancellable asynchronous “tasks”, so the current Xapi task (all operations run in the context of a task) is bound to the Xenopsd task, so cancellation is passed through and progress updates are received.
If the XenAPI call is a storage operation then the “storage access” layer
verifies that the storage objects are in the correct state (SR attached/detached; VDI attached/activated read-only/read-write) invokes the relevant operation in the Storage Manager API (SMAPI) v2 interface; depending on the type of SR: uses the SMAPIv2 to SMAPIv1 converter to generate the necessary command-line to talk to the SMAPIv1 plugin (EXT, NFS, LVM etc) and to execute it uses the SMAPIv2 to SMAPIv3 converter daemon xapi-storage-script to exectute the necessary SMAPIv3 command (GFS2) persists the state of the storage objects (including the result of a VDI.attach call) to persistent storage Internally the SMAPIv1 plugins use privileged access to the Xapi database to directly set fields (e.g. VDI.virtual_size) that would be considered read/only to other clients. The SMAPIv1 plugins also rely on Xapi for
knowledge of all hosts which may access the storage locking of disks within the resource pool safely executing code on other hosts via the “Xapi plugin” mechanism The Xapi database contains Host and VM metadata and is shared pool-wide. The coordinator keeps a copy in memory, and all other nodes remote queries to the coordinator. The database associates each object with a generation count which is used to implement the XenAPI event.next and event.from APIs. The database is routinely asynchronously flushed to disk in XML format. If the “redo-log” is enabled then all database writes are made synchronously as deltas to a shared block device. Without the redo-log, recent updates may be lost if Xapi is killed before a flush.
High-Availability refers to planning for host failure, monitoring host liveness and then following-through on the plans. Xapi defers to an external host liveness monitor called xhad. When xhad confirms that a host has failed – and has been isolated from the storage – then Xapi will restart any VMs which have failed and which have been marked as “protected” by HA. Xapi can also impose admission control to prevent the pool becoming too overloaded to cope with n arbitrary host failures.
The xe CLI is implemented in terms of the XenAPI, but for efficiency the implementation is linked directly into Xapi. The xe program remotes its command-line to Xapi, and Xapi sends back a series of simple commands (prompt for input; print line; fetch file; exit etc).
`,description:"",tags:null,title:"Xapi",uri:"/new-docs/xapi/index.html"},{content:`The XAPI Toolstack manages a cluster of hosts, network switches and storage on behalf of clients such as XenCenter and Xen Orchestra.
The most fundamental concept is of a Resource pool: the whole cluster managed as a single entity. The following diagram shows a cluster of hosts running xapi, all sharing some storage:
At any time, at most one host is known as the pool coordinator (formerly known as “master”) and is responsible for coordination and locking resources within the pool. When a pool is first created a coordinator host is chosen. The coordinator role can be transferred
on user request in an orderly fashion (xe pool-designate-new-master) on user request in an emergency (xe pool-emergency-transition-to-master) automatically if HA is enabled on the cluster. All hosts expose an HTTP, XML-RPC and JSON-RPC interface running on port 80 and with TLS on port 443, but control operations will only be processed on the coordinator host. Attempts to send a control operation to another host will result in a XenAPI redirect error message. For efficiency the following operations are permitted on non-coordinator hosts:
querying performance counters (and their history) connecting to VNC consoles import/export (particularly when disks are on local storage) Since the coordinator host acts as coordinator and lock manager, the other hosts will often talk to the coordinator. Non-coordinator hosts will also talk to each other (over the same HTTP and RPC channels) to
transfer VM memory images (VM migration) mirror disks (storage migration) Note that some types of shared storage (in particular all those using vhd) require coordination for disk GC and coalesce. This coordination is currently done by xapi and hence it is not possible to share this kind of storage between resource pools.
The following diagram shows the software running on a single host. Note that all hosts run the same software (although not necessarily the same version, if we are in the middle of a rolling update).
The XAPI Toolstack expects the host to be running Xen on x86. The Xen hypervisor partitions the host into Domains, some of which can have privileged hardware access, and the rest are unprivileged guests. The XAPI Toolstack normally runs all of its components in the privileged initial domain, Domain 0, also known as “the control domain”. However there is experimental code which supports “driver domains” allowing storage and networking drivers to be isolated in their own domains.
Environment Daemons Interfaces `,description:"",tags:null,title:"High-level architecture",uri:"/new-docs/toolstack/high-level/index.html"},{content:`Xenopsd is the VM manager of the XAPI Toolstack. Xenopsd is responsible for:
Starting, stopping, rebooting, suspending, resuming, migrating VMs. (Hot-)plugging and unplugging devices such as VBDs, VIFs, vGPUs and PCI devices. Setting up VM consoles. Running bootloaders. Setting QoS parameters. Configuring SMBIOS tables. Handling crashes. etc. Check out the full features list.
The code is in ocaml/xenopsd.
Principles Do no harm: Xenopsd should never touch domains/VMs which it hasn’t been asked to manage. This means that it can co-exist with other VM managers such as ‘xl’ and ’libvirt’. Be independent: Xenopsd should be able to work in isolation. In particular the loss of some other component (e.g. the network) should not by itself prevent VMs being managed locally (including shutdown and reboot). Asynchronous by default: Xenopsd exposes task monitoring and offers cancellation for all operations. Xenopsd ensures that the system is always in a manageable state after an operation has been cancelled. Avoid state duplication: where another component owns some state, Xenopsd will always defer to it. We will avoid creating out-of-sync caches of this state. Be debuggable: Xenopsd will expose diagnostic APIs and tools to allow its internal state to be inspected and modified. `,description:"",tags:null,title:"Xenopsd",uri:"/new-docs/xenopsd/index.html"},{content:`The xcp-networkd daemon (hereafter simply called “networkd”) is a component in the xapi toolstack that is responsible for configuring network interfaces and virtual switches (bridges) on a host.
The code is in ocaml/networkd.
Principles Distro-agnostic. Networkd is meant to work on at least CentOS/RHEL as well a Debian/Ubuntu based distros. It therefore should not use any network configuration features specific to those distros.
Stateless. By default, networkd should not maintain any state. If you ask networkd anything about a network interface or bridge, or any other network sub-system property, it will always query the underlying system (e.g. an IP address), rather than returning any cached state. However, if you want networkd to configure networking at host boot time, the you can ask it to remember your configuration you have set for any interface or bridge you choose.
Idempotent. It should be possible to call any networkd function multiple times without breaking things. For example, calling a function to set an IP address on an interface twice in a row should have the same outcome as calling it just once.
Do no harm. Networkd should only configure what you ask it to configure. This means that it can co-exist with other network managers.
Usage Networkd is a daemon that is typically started at host-boot time. In the same way as the other daemons in the xapi toolstack, it is controlled by RPC requests. It typically receives requests from the xapi daemon, on behalf of which it configures host networking.
Networkd’s RCP API is fully described by the network_interface.ml file. The API has two main namespaces: Interface and Bridge, which are implemented in two modules in network_server.ml.
In line with other xapi daemons, all API functions take an argument of type debug_info (a string) as their first argument. The debug string appears in any log lines that are produced as a side effort of calling the function.
Network Interface API The Interface API has functions to query and configure properties of Linux network devices, such as IP addresses, and bringing them up or down. Most Interface functions take a name string as a reference to a network interface as their second argument, which is expected to be the name of the Linux network device. There is also a special function, called Interface.make_config, that is able to configure a number of interfaces at once. It takes an argument called config of type (iface * interface_config_t) list, where iface is an interface name, and interface_config_t is a compound type containing the full configuration for an interface (as far as networkd is able to configure them), currently defined as follows:
type interface_config_t = { ipv4_conf: ipv4; ipv4_gateway: Unix.inet_addr option; ipv6_conf: ipv6; ipv6_gateway: Unix.inet_addr option; ipv4_routes: (Unix.inet_addr * int * Unix.inet_addr) list; dns: Unix.inet_addr list * string list; mtu: int; ethtool_settings: (string * string) list; ethtool_offload: (string * string) list; persistent_i: bool; }When the function returns, it should have completely configured the interface, and have brought it up. The idempotency principle applies to this function, which means that it can be used to successively modify interface properties; any property that has not changed will effectively be ignored. In fact, Interface.make_config is the main function that xapi uses to configure interfaces, e.g. as a result of a PIF.plug or a PIF.reconfigure_ip call.
Also note the persistent property in the interface config. When an interface is made “persistent”, this means that any configuration that is set on it is remembered by networkd, and the interface config is written to disk. When networkd is started, it will read the persistent config and call Interface.make_config on it in order to apply it (see Startup below).
The full networkd API should be documented separately somewhere on this site.
Bridge API The Bridge API functions are all about the management of virtual switches, also known as “bridges”. The shape of the Bridge API roughly follows that of the Open vSwitch in that it treats a bridge as a collection of “ports”, where a port can contain one or more “interfaces”.
NIC bonding and VLANs are all configured on the Bridge level. There are functions for creating and destroying bridges, adding and removing ports, and configuring bonds and VLANs. Like interfaces, bridges and ports are addressed by name in the Bridge functions. Analogous to the Interface function with the same name, there is a Bridge.make_config function, and bridges can be made persistent.
type port_config_t = { interfaces: iface list; bond_properties: (string * string) list; bond_mac: string option; } type bridge_config_t = { ports: (port * port_config_t) list; vlan: (bridge * int) option; bridge_mac: string option; other_config: (string * string) list; persistent_b: bool; }Backends Networkd currently has two different backends: the “Linux bridge” backend and the “Open vSwitch” backend. The former is the “classic” backend based on the bridge module that is available in the Linux kernel, plus additional standard Linux functionality for NIC bonding and VLANs. The latter backend is newer and uses the Open vSwitch (OVS) for bridging as well as other functionality. Which backend is currently in use is defined by the file /etc/xensource/network.conf, which is read by networkd when it starts. The choice of backend (currently) only affects the Bridge API: every function in it has a separate implementation for each backend.
Low-level Interfaces Networkd uses standard networking commands and interfaces that are available in most modern Linux distros, rather than relying on any distro-specific network tools (see the distro-agnostic principle). These are tools such as ip (iproute2), dhclient and brctl, as well as the sysfs files system, and netlink sockets. To control the OVS, the ovs-* command line tools are used. All low-level functions are called from network_utils.ml.
Configuration on Startup Networkd, periodically as well as on shutdown, writes the current configuration of all bridges and interfaces (see above) in a JSON format to a file called networkd.db (currently in /var/lib/xcp). The contents of the file are completely described by the following type:
type config_t = { interface_config: (iface * interface_config_t) list; bridge_config: (bridge * bridge_config_t) list; gateway_interface: iface option; dns_interface: iface option; }The gateway_interface and dns_interface in the config are global host-level options to define from which interfaces the default gateway and DNS configuration is taken. This is especially important when multiple interfaces are configured by DHCP.
When networkd starts up, it first reads network.conf to determine the network backend. It subsequently attempts to parse networkd.db, and tries to call Bridge.make_config and Interface.make_config on it, with a special options to only apply the config for persistent bridges and interfaces, as well as bridges related to those (for example, if a VLAN bridge is configured, then also its parent bridge must be configured).
Networkd also supports upgrades from older versions of XenServer that used a network configuration script called interface-configure. If networkd.db is not found on startup, then networkd attempts to call this tool (via the /etc/init.d/management-interface script) in order to set up networking at boot time. This is normally followed immediately by a call from xapi instructing networkd to take over.
Finally, if no network config (old or new) is found on disk at all, networkd looks for a XenServer “firstboot” data file, which is written by XenServer’s host installer, and tries to apply it to set up the management interface.
Monitoring Besides the ability to configure bridges and network interfaces, networkd has facilities for monitoring interfaces and bonds. When networkd starts, a monitor thread is started, which does several things (see network_monitor_thread.ml):
Every 5 seconds, it gathers send/receive counters and link state of all network interfaces. It then writes these stats to a shared-memory file, to be picked up by other components such as xcp-rrdd and xapi (see documentation about “xenostats” elsewhere). It monitors NIC bonds, and sends alerts through xapi in case of link state changes within a bond. It uses ip monitor address to watch for an IP address changes, and if so, it calls xapi (Host.signal_networking_change) for it to update the IP addresses of the PIFs in its database that were configured by DHCP. `,description:"",tags:null,title:"Networkd",uri:"/new-docs/xcp-networkd/index.html"},{content:" Disaster Recovery High-Availability Snapshots vGPU Xapi Storage Migration ",description:"",tags:null,title:"Features",uri:"/new-docs/toolstack/features/index.html"},{content:`Helpful guides for xapi developers.
How to add.... Adding a Class to the API Adding a field to the API Adding a function to the API Adding a XenAPI extension `,description:"",tags:null,title:"Guides",uri:"/new-docs/xapi/guides/index.html"},{content:`This document describes how to add a new class to the data model that defines the Xen Server API. It complements two other documents that describe how to extend an existing class:
Adding a Field Adding a Function As a running example, we will use the addition of a class that is part of the design for the PVS Direct feature. PVS Direct introduces proxies that serve VMs with disk images. This class was added via commit CP-16939 to Xen API.
Example: PVS_server In the world of Xen Server, each important concept like a virtual machine, interface, or users is represented by a class in the data model. A class defines methods and instance variables. At runtime, all class instances are held in an in-memory database. For example, part of [PVS Direct] is a class PVS_server, representing a resource that provides block-level data for virtual machines. The design document defines it to have the following important properties:
Fields (string set) addresses (RO/constructor) IPv4 addresses of the server.
(int) first_port (RO/constructor) First UDP port accepted by the server.
(int) last_port (RO/constructor) Last UDP port accepted by the server.
(PVS_farm ref) farm (RO/constructor) Link to the farm that this server is included in. A PVS_server object must always have a valid farm reference; the PVS_server will be automatically GC’ed by xapi if the associated PVS_farm object is removed.
(string) uuid (R0/runtime) Unique identifier/object reference. Allocated by the server.
Methods (or Functions) (PVS_server ref) introduce (string set addresses, int first_port, int last_port, PVS_farm ref farm) Introduce a new PVS server into the farm. Allowed at any time, even when proxies are in use. The proxies will be updated automatically.
(void) forget (PVS_server ref self) Remove a PVS server from the farm. Allowed at any time, even when proxies are in use. The proxies will be updated automatically.
Implementation Overview The implementation of a class is distributed over several files:
ocaml/idl/datamodel.ml – central class definition ocaml/idl/datamodel_types.ml – definition of releases ocaml/xapi/cli_frontend.ml – declaration of CLI operations ocaml/xapi/cli_operations.ml – implementation of CLI operations ocaml/xapi/records.ml – getters and setters ocaml/xapi/OMakefile – refers to xapi_pvs_farm.ml ocaml/xapi/api_server.ml – refers to xapi_pvs_farm.ml ocaml/xapi/message_forwarding.ml ocaml/xapi/xapi_pvs_farm.ml – implementation of methods, new file Data Model The data model ocaml/idl/datamodel.ml defines the class. To keep the name space tidy, most helper functions are grouped into an internal module:
(* datamodel.ml *) let schema_minor_vsn = 103 (* line 21 -- increment this *) let _pvs_farm = "PVS_farm" (* line 153 *) module PVS_farm = struct (* line 8658 *) let lifecycle = [Prototyped, rel_dundee_plus, ""] let introduce = call ~name:"introduce" ~doc:"Introduce new PVS farm" ~result:(Ref _pvs_farm, "the new PVS farm") ~params: [ String,"name","name of the PVS farm" ] ~lifecycle ~allowed_roles:_R_POOL_OP () let forget = call ~name:"forget" ~doc:"Remove a farm's meta data" ~params: [ Ref _pvs_farm, "self", "this PVS farm" ] ~errs:[ Api_errors.pvs_farm_contains_running_proxies; Api_errors.pvs_farm_contains_servers; ] ~lifecycle ~allowed_roles:_R_POOL_OP () let set_name = call ~name:"set_name" ~doc:"Update the name of the PVS farm" ~params: [ Ref _pvs_farm, "self", "this PVS farm" ; String, "value", "name to be used" ] ~lifecycle ~allowed_roles:_R_POOL_OP () let add_cache_storage = call ~name:"add_cache_storage" ~doc:"Add a cache SR for the proxies on the farm" ~params: [ Ref _pvs_farm, "self", "this PVS farm" ; Ref _sr, "value", "SR to be used" ] ~lifecycle ~allowed_roles:_R_POOL_OP () let remove_cache_storage = call ~name:"remove_cache_storage" ~doc:"Remove a cache SR for the proxies on the farm" ~params: [ Ref _pvs_farm, "self", "this PVS farm" ; Ref _sr, "value", "SR to be removed" ] ~lifecycle ~allowed_roles:_R_POOL_OP () let obj = let null_str = Some (VString "") in let null_set = Some (VSet []) in create_obj (* <---- creates class *) ~name: _pvs_farm ~descr:"machines serving blocks of data for provisioning VMs" ~doccomments:[] ~gen_constructor_destructor:false ~gen_events:true ~in_db:true ~lifecycle ~persist:PersistEverything ~in_oss_since:None ~messages_default_allowed_roles:_R_POOL_OP ~contents: [ uid _pvs_farm ~lifecycle ; field ~qualifier:StaticRO ~lifecycle ~ty:String "name" ~default_value:null_str "Name of the PVS farm. Must match name configured in PVS" ; field ~qualifier:DynamicRO ~lifecycle ~ty:(Set (Ref _sr)) "cache_storage" ~default_value:null_set ~ignore_foreign_key:true "The SR used by PVS proxy for the cache" ; field ~qualifier:DynamicRO ~lifecycle ~ty:(Set (Ref _pvs_server)) "servers" "The set of PVS servers in the farm" ; field ~qualifier:DynamicRO ~lifecycle ~ty:(Set (Ref _pvs_proxy)) "proxies" "The set of proxies associated with the farm" ] ~messages: [ introduce ; forget ; set_name ; add_cache_storage ; remove_cache_storage ] () end let pvs_farm = PVS_farm.obj The class is defined by a call to create_obj and it defines the fields and messages (methods) belonging to the class. Each field has a name, a type, and some meta information. Likewise, each message (or method) is created by call that describes its parameters.
The PVS_farm has additional getter and setter methods for accessing its fields. These are not declared here as part of the messages but are automatically generated.
To make sure the new class is actually used, it is important to enter it into two lists:
(* datamodel.ml *) let all_system = (* line 8917 *) [ ... vgpu_type; pvs_farm; ... ] let expose_get_all_messages_for = [ (* line 9097 *) ... _pvs_farm; _pvs_server; _pvs_proxy; When a field refers to another object that itself refers back to it, these two need to be entered into the all_relations list. For example, _pvs_server refers to a _pvs_farm value via "farm", which, in turn, refers to the _pvs_server value via its "servers" field.
let all_relations = [ (* ... *) (_sr, "introduced_by"), (_dr_task, "introduced_SRs"); (_pvs_server, "farm"), (_pvs_farm, "servers"); (_pvs_proxy, "farm"), (_pvs_farm, "proxies"); ] CLI Conventions The CLI provides access to objects from the command line. The following conventions exist for naming fields:
A field in the data model uses an underscore (_) but a hyphen (-) in the CLI: what is cache_storage in the data model becomes cache-storage in the CLI.
When a field contains a reference or multiple, like proxies, it becomes proxy-uuids in the CLI because references are always referred to by their UUID.
CLI Getters and Setters All fields can be read from the CLI and some fields can also be set via the CLI. These getters and setters are mostly generated automatically and need to be connected to the CLI through a function in ocaml/xapi/records.ml. Note that field names here use the naming convention for the CLI:
(* ocaml/xapi/records.ml *) let pvs_farm_record rpc session_id pvs_farm = let _ref = ref pvs_farm in let empty_record = ToGet (fun () -> Client.PVS_farm.get_record rpc session_id !_ref) in let record = ref empty_record in let x () = lzy_get record in { setref = (fun r -> _ref := r ; record := empty_record) ; setrefrec = (fun (a,b) -> _ref := a; record := Got b) ; record = x ; getref = (fun () -> !_ref) ; fields= [ make_field ~name:"uuid" ~get:(fun () -> (x ()).API.pVS_farm_uuid) () ; make_field ~name:"name" ~get:(fun () -> (x ()).API.pVS_farm_name) ~set:(fun name -> Client.PVS_farm.set_name rpc session_id !_ref name) () ; make_field ~name:"cache-storage" ~get:(fun () -> (x ()).API.pVS_farm_cache_storage |> List.map get_uuid_from_ref |> String.concat "; ") ~add_to_set:(fun sr_uuid -> let sr = Client.SR.get_by_uuid rpc session_id sr_uuid in Client.PVS_farm.add_cache_storage rpc session_id !_ref sr) ~remove_from_set:(fun sr_uuid -> let sr = Client.SR.get_by_uuid rpc session_id sr_uuid in Client.PVS_farm.remove_cache_storage rpc session_id !_ref sr) () ; make_field ~name:"server-uuids" ~get:(fun () -> (x ()).API.pVS_farm_servers |> List.map get_uuid_from_ref |> String.concat "; ") ~get_set:(fun () -> (x ()).API.pVS_farm_servers |> List.map get_uuid_from_ref) () ; make_field ~name:"proxy-uuids" ~get:(fun () -> (x ()).API.pVS_farm_proxies |> List.map get_uuid_from_ref |> String.concat "; ") ~get_set:(fun () -> (x ()).API.pVS_farm_proxies |> List.map get_uuid_from_ref) () ] } CLI Interface to Methods Methods accessible from the CLI are declared in ocaml/xapi/cli_frontend.ml. Each declaration refers to the real implementation of the method, like Cli_operations.PVS_far.introduce:
(* cli_frontend.ml *) let rec cmdtable_data : (string*cmd_spec) list = (* ... *) "pvs-farm-introduce", { reqd=["name"]; optn=[]; help="Introduce new PVS farm"; implementation=No_fd Cli_operations.PVS_farm.introduce; flags=[]; }; "pvs-farm-forget", { reqd=["uuid"]; optn=[]; help="Forget a PVS farm"; implementation=No_fd Cli_operations.PVS_farm.forget; flags=[]; }; CLI Implementation of Methods Each CLI operation that is not a getter or setter has an implementation in cli_operations.ml which is implemented in terms of the real implementation:
(* cli_operations.ml *) module PVS_farm = struct let introduce printer rpc session_id params = let name = List.assoc "name" params in let ref = Client.PVS_farm.introduce ~rpc ~session_id ~name in let uuid = Client.PVS_farm.get_uuid rpc session_id ref in printer (Cli_printer.PList [uuid]) let forget printer rpc session_id params = let uuid = List.assoc "uuid" params in let ref = Client.PVS_farm.get_by_uuid ~rpc ~session_id ~uuid in Client.PVS_farm.forget rpc session_id ref end Fields that should show up in the CLI interface by default are declared in the gen_cmds value:
(* cli_operations.ml *) let gen_cmds rpc session_id = let mk = make_param_funs in List.concat [ (*...*) ; Client.Pool.(mk get_all get_all_records_where get_by_uuid pool_record "pool" [] ["uuid";"name-label";"name-description";"master" ;"default-SR"] rpc session_id) ; Client.PVS_farm.(mk get_all get_all_records_where get_by_uuid pvs_farm_record "pvs-farm" [] ["uuid";"name";"cache-storage";"server-uuids"] rpc session_id) Error messages Error messages used by an implementation are introduced in two files:
(* ocaml/xapi-consts/api_errors.ml *) let pvs_farm_contains_running_proxies = "PVS_FARM_CONTAINS_RUNNING_PROXIES" let pvs_farm_contains_servers = "PVS_FARM_CONTAINS_SERVERS" let pvs_farm_sr_already_added = "PVS_FARM_SR_ALREADY_ADDED" let pvs_farm_sr_is_in_use = "PVS_FARM_SR_IS_IN_USE" let sr_not_in_pvs_farm = "SR_NOT_IN_PVS_FARM" let pvs_farm_cant_set_name = "PVS_FARM_CANT_SET_NAME" (* ocaml/idl/datamodel.ml *) (* PVS errors *) error Api_errors.pvs_farm_contains_running_proxies ["proxies"] ~doc:"The PVS farm contains running proxies and cannot be forgotten." (); error Api_errors.pvs_farm_contains_servers ["servers"] ~doc:"The PVS farm contains servers and cannot be forgotten." (); error Api_errors.pvs_farm_sr_already_added ["farm"; "SR"] ~doc:"Trying to add a cache SR that is already associated with the farm" (); error Api_errors.sr_not_in_pvs_farm ["farm"; "SR"] ~doc:"The SR is not associated with the farm." (); error Api_errors.pvs_farm_sr_is_in_use ["farm"; "SR"] ~doc:"The SR is in use by the farm and cannot be removed." (); error Api_errors.pvs_farm_cant_set_name ["farm"] ~doc:"The name of the farm can't be set while proxies are active." () Method Implementation The implementation of methods lives in a module in ocaml/xapi:
(* ocaml/xapi/api_server.ml *) module PVS_farm = Xapi_pvs_farm The file below is typically a new file and needs to be added to ocaml/xapi/OMakefile.
(* ocaml/xapi/xapi_pvs_farm.ml *) module D = Debug.Make(struct let name = "xapi_pvs_farm" end) module E = Api_errors let api_error msg xs = raise (E.Server_error (msg, xs)) let introduce ~__context ~name = let pvs_farm = Ref.make () in let uuid = Uuid.to_string (Uuid.make_uuid ()) in Db.PVS_farm.create ~__context ~ref:pvs_farm ~uuid ~name ~cache_storage:[]; pvs_farm (* ... *) Messages received on a slave host may or may not be executed there. In the simple case, each methods executes locally:
(* ocaml/xapi/message_forwarding.ml *) module PVS_farm = struct let introduce ~__context ~name = info "PVS_farm.introduce %s" name; Local.PVS_farm.introduce ~__context ~name let forget ~__context ~self = info "PVS_farm.forget"; Local.PVS_farm.forget ~__context ~self let set_name ~__context ~self ~value = info "PVS_farm.set_name %s" value; Local.PVS_farm.set_name ~__context ~self ~value let add_cache_storage ~__context ~self ~value = info "PVS_farm.add_cache_storage"; Local.PVS_farm.add_cache_storage ~__context ~self ~value let remove_cache_storage ~__context ~self ~value = info "PVS_farm.remove_cache_storage"; Local.PVS_farm.remove_cache_storage ~__context ~self ~value end `,description:"",tags:null,title:"Adding a Class to the API",uri:"/new-docs/xapi/guides/howtos/add-class/index.html"},{content:`This page describes how to add a field to XenAPI. A field is a parameter of a class that can be used in functions and read from the API.
Bumping the database schema version Whenever a field is added to or removed from the API, its schema version needs to be increased. XAPI needs this fundamental procedure in order to be able to detect that an automatic database upgrade is necessary or to find out that the new schema is incompatible with the existing database. If the schema version is not bumped, XAPI will start failing in unpredictable ways. Note that bumping the version is not necessary when adding functions, only when adding fields.
The current version number is kept at the top of the file ocaml/idl/datamodel_common.ml in the variables schema_major_vsn and schema_minor_vsn, of which only the latter should be incremented (the major version only exists for historical reasons). When moving to a new XenServer release, also update the variable last_release_schema_minor_vsn to the schema version of the last release. To keep track of the schema versions of recent XenServer releases, the file contains variables for these, such as miami_release_schema_minor_vsn. After starting a new version of Xapi on an existing server, the database is automatically upgraded if the schema version of the existing database matches the value of last_release_schema_*_vsn in the new Xapi.
As an example, the patch below shows how the schema version was bumped when the new API fields used for ActiveDirectory integration were added:
--- a/ocaml/idl/datamodel.ml Tue Nov 11 16:17:48 2008 +0000 +++ b/ocaml/idl/datamodel.ml Tue Nov 11 15:53:29 2008 +0000 @@ -15,17 +15,20 @@ open Datamodel_types open Datamodel_types (* IMPORTANT: Please bump schema vsn if you change/add/remove a _field_. You do not have to dump vsn if you change/add/remove a message *) let schema_major_vsn = 5 -let schema_minor_vsn = 55 +let schema_minor_vsn = 56 (* Historical schema versions just in case this is useful later *) let rio_schema_major_vsn = 5 let rio_schema_minor_vsn = 19 +let miami_release_schema_major_vsn = 5 +let miami_release_schema_minor_vsn = 35 + (* the schema vsn of the last release: used to determine whether we can upgrade or not.. *) let last_release_schema_major_vsn = 5 -let last_release_schema_minor_vsn = 35 +let last_release_schema_minor_vsn = 55 Setting the schema hash In the ocaml/idl/schematest.ml there is the last_known_schema_hash This needs to be updated to be the next hash after the schema version was bumped. Get the new hash by running make test and you will receive the correct hash in the error message.
Adding the new field to some existing class ocaml/idl/datamodel.ml Add a new “field” line to the class in the file ocaml/idl/datamodel.ml or ocaml/idl/datamodel_[class].ml. The new field might require a suitable default value. This default value is used in case the user does not provide a value for the field.
A field has a number of parameters:
The lifecycle parameter, which shows how the field has evolved over time. The qualifier parameter, which controls access to the field. The following values are possible: Value Meaning StaticRO Field is set statically at install-time. DynamicRO Field is computed dynamically at run time. RW Field is read/write. The ty parameter for the type of the field. The default_value parameter. The name of the field. A documentation string. Example of a field in the pool class:
field ~lifecycle:[Published, rel_orlando, "Controls whether HA is enabled"] ~qualifier:DynamicRO ~ty:Bool ~default_value:(Some (VBool false)) "ha_enabled" "true if HA is enabled on the pool, false otherwise"; See datamodel_types.ml for information about other parameters.
Changing Constructors Adding a field would change the constructors for the class – functions Db.*.create – and therefore, any references to these in the code need to be updated. In the example, the argument ~ha_enabled:false should be added to any call to Db.Pool.create.
Examples of where these calls can be found is in ocaml/tests/common/test_common.ml and ocaml/xapi/xapi_[class].ml.
CLI Records If you want this field to show up in the CLI (which you probably do), you will also need to modify the Records module, in the file ocaml/xapi-cli-server/records.ml. Find the record function for the class which you have modified, add a new entry to the fields list using make_field. This type can be found in the same file.
The only required parameters are name and get (and unit, of course ). If your field is a map or set, then you will need to pass in get_{map,set}, and optionally set_{map,set}, if it is a RW field. The hidden parameter is useful if you don’t want this field to show up in a *_params_list call. As an example, here is a field that we’ve just added to the SM class:
make_field ~name:"versioned-capabilities" ~get:(fun () -> Record_util.s2sm_to_string "; " (x ()).API.sM_versioned_capabilities) ~get_map:(fun () -> (x ()).API.sM_versioned_capabilities) ~hidden:true (); Testing The new fields can be tested by copying the newly compiled xapi binary to a test box. After the new xapi service is started, the file /var/log/xensource.log in the test box should contain a few lines reporting the successful upgrade of the metadata schema in the test box:
[...|xapi] Db has schema major_vsn=5, minor_vsn=57 (current is 5 58) (last is 5 57) [...|xapi] Database schema version is that of last release: attempting upgrade [...|sql] attempting to restore database from /var/xapi/state.db [...|sql] finished parsing xml [...|sql] writing db as xml to file '/var/xapi/state.db'. [...|xapi] Database upgrade complete, restarting to use new db Making this field accessible as a CLI attribute XenAPI functions to get and set the value of the new field are generated automatically. It requires some extra work, however, to enable such operations in the CLI.
The CLI has commands such as host-param-list and host-param-get. To make a new field accessible by these commands, the file xapi-cli-server/records.ml needs to be edited. For the pool.ha-enabled field, the pool_record function in this file contains the following (note the convention to replace underscores by hyphens in the CLI):
let pool_record rpc session_id pool = ... [ ... make_field ~name:"ha-enabled" ~get:(fun () -> string_of_bool (x ()).API.pool_ha_enabled) (); ... ]} NB: the ~get parameter must return a string so include a relevant function to convert the type of the field into a string i.e. string_of_bool
See xapi-cli-server/records.ml for examples of handling field types other than Bool.
`,description:"",tags:null,title:"Adding a field to the API",uri:"/new-docs/xapi/guides/howtos/add-field/index.html"},{content:`This page describes how to add a function to XenAPI.
Add message to API The file idl/datamodel.ml is a description of the API, from which the marshalling and handler code is generated.
In this file, the create_obj function is used to define a class which may contain fields and support operations (known as “messages”). For example, the identifier host is defined using create_obj to encapsulate the operations which can be performed on a host.
In order to add a function to the API, we need to add a message to an existing class. This entails adding a function in idl/datamodel.ml or one of the other datamodel files to describe the new message and adding it to the class’s list of messages. In this example, we are adding to idl/datamodel_host.ml.
The function to describe the new message will look something like the following:
let host_price_of = call ~flags:[\`Session] ~name:"price_of" ~in_oss_since:None ~in_product_since:rel_orlando ~params:[(Ref _host, "host", "The host containing the price information"); (String, "item", "The item whose price is queried")] ~result:(Float, "The price of the item") ~doc:"Returns the price of a named item." ~allowed_roles:_R_POOL_OP () By convention, the name of the function is formed from the name of the class and the name of the message: host and price_of, in the example. An entry for host_price_of is added to the messages of the host class:
let host = create_obj ... ~messages: [... host_price_of; ] ... The parameters passed to call are all optional (except ~name and ~in_product_since).
The ~flags parameter is used to set conditions for the use of the message. For example, \`Session is used to indicate that the call must be made in the presence of an existing session.
The value of the ~in_product_since parameter is a string taken from idl/datamodel_types.ml indicates the XenServer release in which this message was first introduced.
The ~params parameter describes a list of the formal parameters of the message. Each parameter is described by a triple. The first component of the triple is the type (from type ty in idl/datamodel_types.ml); the second is the name of the parameter, and the third is a human-readable description of the parameter. The first triple in the list is conventionally the instance of the class on which the message will operate. In the example, this is a reference to the host.
Similarly, the ~result describes the message’s return type, although this is permitted to merely be a single value rather than a list of values. If no ~result is specified, the default is unit.
The ~doc parameter describes what the message is doing.
The bool ~hide_from_docs parameter prevents the message from being included in the documentation when generated.
The bool ~pool_internal parameter is used to indicate if the message should be callable by external systems or only internal hosts.
The ~errs parameter is a list of possible exceptions that the message can raise.
The parameter ~lifecycle takes in an array of (Status, version, doc) to indicate the lifecycle of the message type. This takes over from ~in_oss_since which indicated the release that the message type was introduced. NOTE: Leave this parameter empty, it will be populated on build.
The ~allowed_roles parameter is used for access control (see below).
Compiling xen-api.(hg|git) will cause the code corresponding to this message to be generated and output in ocaml/xapi/server.ml. In the example above, a section handling an incoming call host.price_of appeared in ocaml/xapi/server.ml. However, after this was generated, the rest of the build failed because this call expects a price_of function in the Host object.
Expected values in parameter ~in_product_since In the example above, the value of the parameter ~in_product_since informs that the message host_price_of was added during the rel_orlando release cycle. If a new release cycle is required, then it needs to be added in the file idl/datamodel_types.ml. The patch below shows how the new rel_george release identifier was added. Any class, message, etc. added during the rel_george release cycle should contain ~in_product_since:rel_george entries. (obs: the release and upgrade infrastructure can handle only one new rel_* identifier – in this case, rel_george – in each release)
--- a/ocaml/idl/datamodel_types.ml Tue Nov 11 15:17:48 2008 +0000 +++ b/ocaml/idl/datamodel_types.ml Tue Nov 11 15:53:29 2008 +0000 @@ -27,14 +27,13 @@ (* useful constants for product vsn tracking *) let oss_since_303 = Some "3.0.3" +let rel_george = "george" let rel_orlando = "orlando" let rel_orlando_update_1 = "orlando-update-1" let rel_symc = "symc" let rel_miami = "miami" let rel_rio = "rio" -let release_order = [engp:rel_rio; rel_miami; rel_symc; rel_orlando; rel_orlando_update_1] +let release_order = [engp:rel_rio; rel_miami; rel_symc; rel_orlando; rel_orlando_update_1; rel_george] Update expose_get_all_messages_for list If you are adding a new class, do not forget to add your new class _name to the expose_get_all_messages_for list, at the bottom of datamodel.ml, in order to have automatically generated get_all and get_all_records functions attached to it.
Update the RBAC field containing the roles expected to use the new API call After the RBAC integration, Xapi provides by default a set of static roles associated to the most common subject tasks.
The api calls associated with each role are defined by a new ~allowed_roles parameter in each api call, which specifies the list of static roles that should be able to execute the call. The possible roles for this list is one of the following names, defined in datamodel.ml:
role_pool_admin role_pool_operator role_vm_power_admin role_vm_admin role_vm_operator role_read_only So, for instance,
~allowed_roles:[role_pool_admin,role_pool_operator] (* this is not the recommended usage, see example below *) would be a valid list (though it is not the recommended way of using allowed_roles, see below), meaning that subjects belonging to either role_pool_admin or role_pool_operator can execute the api call.
The RBAC requirements define a policy where the roles in the list above are supposed to be totally-ordered by the set of api-calls associated with each of them. That means that any api-call allowed to role_pool_operator should also be in role_pool_admin; any api-call allowed to role_vm_power_admin should also be in role_pool_operator and also in role_pool_admin; and so on. Datamodel.ml provides shortcuts for expressing these totally-ordered set of roles policy associated with each api-call:
_R_POOL_ADMIN, equivalent to [role_pool_admin] _R_POOL_OP, equivalent to [role_pool_admin,role_pool_operator] _R_VM_POWER_ADMIN, equivalent to [role_pool_admin,role_pool_operator,role_vm_power_admin] _R_VM_ADMIN, equivalent to [role_pool_admin,role_pool_operator,role_vm_power_admin,role_vm_admin] _R_VM_OP, equivalent to [role_pool_admin,role_pool_operator,role_vm_power_admin,role_vm_admin,role_vm_op] _R_READ_ONLY, equivalent to [role_pool_admin,role_pool_operator,role_vm_power_admin,role_vm_admin,role_vm_op,role_read_only] The ~allowed_roles parameter should use one of the shortcuts in the list above, instead of directly using a list of roles, because the shortcuts above make sure that the roles in the list are in a total order regarding the api-calls permission sets. Creating an api-call with e.g. allowed_roles:[role_pool_admin,role_vm_admin] would be wrong, because that would mean that a pool_operator cannot execute the api-call that a vm_admin can, breaking the total-order policy expected in the RBAC 1.0 implementation. In the future, this requirement might be relaxed.
So, the example above should instead be used as:
~allowed_roles:_R_POOL_OP (* recommended usage via pre-defined totally-ordered role lists *) and so on.
How to determine the correct role of a new api-call: if only xapi should execute the api-call, ie. it is an internal call: _R_POOL_ADMIN if it is related to subject, role, external-authentication: _R_POOL_ADMIN if it is related to accessing Dom0 (via console, ssh, whatever): _R_POOL_ADMIN if it is related to the pool object: R_POOL_OP if it is related to the host object, licenses, backups, physical devices: _R_POOL_OP if it is related to managing VM memory, snapshot/checkpoint, migration: _R_VM_POWER_ADMIN if it is related to creating, destroying, cloning, importing/exporting VMs: _R_VM_ADMIN if it is related to starting, stopping, pausing etc VMs or otherwise accessing/manipulating VMs: _R_VM_OP if it is related to being able to login, manipulate own tasks and read values only: _R_READ_ONLY Update message forwarding The “message forwarding” layer describes the policy of whether an incoming API call should be forwarded to another host (such as another member of the pool) or processed on the host which receives the call. This policy may be non-trivial to describe and so cannot be auto-generated from the data model.
In xapi/message_forwarding.ml, add a function to the relevant module to describe this policy. In the running example, we add the following function to the Host module:
let price_of ~__context ~host ~item = info "Host.price_of for item %s" item; let local_fn = Local.Host.price_of ~host ~item in do_op_on ~local_fn ~__context ~host (fun session_id rpc -> Client.Host.price_of ~rpc ~session_id ~host ~item) After the ~__context parameter, the parameters of this new function should match the parameters we specified for the message. In this case, that is the host and the item to query the price of.
The do_op_on function takes a function to execute locally and a function to execute remotely and performs one of these operations depending on whether the given host is the local host.
The local function references Local.Host.price_of, which is a function we will write in the next step.
Implement the function Now we write the function to perform the logic behind the new API call. For a host-based call, this will reside in xapi/xapi_host.ml. For other classes, other files with similar names are used.
We add the following function to xapi/xapi_host.ml:
let price_of ~__context ~host ~item = if item = "fish" then 3.14 else 0.00 We also need to add the function to the interface xapi/xapi_host.mli:
val price_of : __context:Context.t -> host:API.ref_host -> item:string -> float Congratulations, you’ve added a function to the API!
Add the operation to the CLI Edit xapi-cli-server/cli_frontend.ml. Add a block to the definition of cmdtable_data as in the following example:
"host-price-of", { reqd=["host-uuid"; "item"]; optn=[]; help="Find out the price of an item on a certain host."; implementation= No_fd Cli_operations.host_price_of; flags=[]; }; Include here the following:
The names of required (reqd) and optional (optn) parameters.
A description to be displayed when calling xe help <cmd> in the help field.
The implementation should use With_fd if any communication with the client is necessary (for example, showing the user a warning, sending the contents of a file, etc.) Otherwise, No_fd can be used as above.
The flags field can be used to set special options:
Vm_selectors: adds a “vm” parameter for the name of a VM (rather than a UUID) Host_selectors: adds a “host” parameter for the name of a host (rather than a UUID) Standard: includes the command in the list of common commands displayed by xe help Neverforward: Hidden: Deprecated of string list: Now we must implement Cli_operations.host_price_of. This is done in xapi-cli-server/cli_operations.ml. This function typically extracts the parameters and forwards them to the internal implementation of the function. Other arbitrary code is permitted. For example:
let host_price_of printer rpc session_id params = let host = Client.Host.get_by_uuid rpc session_id (List.assoc "host-uuid" params) in let item = List.assoc "item" params in let price = string_of_float (Client.Host.price_of ~rpc ~session_id ~host ~item) in printer (Cli_printer.PList [price]) Tab Completion in the CLI The CLI features tab completion for many of its commands’ parameters. Tab completion is implemented in the file ocaml/xe-cli/bash-completion, which is installed on the host as /etc/bash_completion.d/cli, and is done on a parameter-name rather than on a command-name basis. The main portion of the bash-completion file is a case statement that contains a section for each of the parameters that benefit from completion. There is also an entry that catches all parameter names ending at -uuid, and performs an automatic lookup of suitable UUIDs. The host-uuid parameter of our new host-price-of command therefore automatically gains completion capabilities.
Executing the CLI operation Recompile xapi with the changes described above and install it on a test machine.
Execute the following command to see if the function exists:
xe help host-price-of Invoke the function itself with the following command:
xe host-price-of host-uuid=<tab> item=fish and you should find out the price of fish.
`,description:"",tags:null,title:"Adding a function to the API",uri:"/new-docs/xapi/guides/howtos/add-function/index.html"},{content:`A XenAPI extension is a new RPC which is implemented as a separate executable (i.e. it is not part of xapi) but which still benefits from xapi parameter type-checking, multi-language stub generation, documentation generation, authentication etc. An extension can be backported to previous versions by simply adding the implementation, without having to recompile xapi itself.
A XenAPI extension is in two parts:
a declaration in the xapi datamodel. This must use the ~forward_to:(Extension "filename") parameter. The filename must be unique, and should be the same as the XenAPI call name. an implementation executable in the dom0 filesystem with path /etc/xapi.d/extensions/filename To define an extension First write the declaration in the datamodel. The act of specifying the types and writing the documentation will help clarify the intended meaning of the call.
Second create a prototype of your implementation and put an executable file in /etc/xapi.d/extensions/filename. The calling convention is:
the file must be executable xapi will parse the XMLRPC call arguments received over the network and check the session_id is valid xapi will execute the named executable the XMLRPC call arguments will be sent to the executable on stdin and stdin will be closed afterwards the executable will run and print an XMLRPC response on stdout xapi will read the response and return it to the client. See the basic example.
Second make a pull request containing only the datamodel definitions (it is not necessary to include the prototype too). This will attract review comments which will help you improve your API further. Once the pull request is merged, then the API call name and extension are officially yours and you may use them on any xapi version which supports the extension mechanism.
Packaging your extension Your extension /etc/xapi.d/extensions/filename (and dependencies) should be packaged for your target distribution (for XenServer dom0 this would be a CentOS RPM). Once the package is unpacked on the target machine, the extension should be immediately callable via the XenAPI, provided the xapi version supports the extension mechanism. Note the xapi version does not need to know about the specific extension in advance: it will always look in /etc/xapi.d/extensions/ for all RPC calls whose name it does not recognise.
Limitations On type-checking
if the xapi version is new enough to know about your specific extension: xapi will type-check the call arguments for you if the xapi version is too old to know about your specific extension: the extension will still be callable but the arguments will not be type-checked. On access control
if the xapi version is new enough to know about your specific extension: you can declare that a user must have a particular role (e.g. ‘VM admin’) if the xapi version is too old to know about your specific extension: the extension will still be callable but the client must have the ‘Pool admin’ role. Since a xapi which knows about your specific extension is stricter than an older xapi, it’s a good idea to develop against the new xapi and then test older xapi versions later.
`,description:"",tags:null,title:"Adding a XenAPI extension",uri:"/new-docs/xapi/guides/howtos/add-api-extension/index.html"},{content:`Xenopsd instances run on a host and manage VMs on behalf of clients. This picture shows 3 different Xenopsd instances: 2 named “xenopsd-xc” and 1 named “xenopsd-xenlight”.
Each instance is responsible for managing a disjoint set of VMs. Clients should never ask more than one Xenopsd to manage the same VM. Managing a VM means:
handling start/shutdown/suspend/resume/migrate/reboot allowing devices (disks, nics, PCI cards, vCPUs etc) to be manipulated providing updates to clients when things change (reboots, console becomes available, guest agent says something etc). For a full list of features, consult the features list.
Each Xenopsd instance has a unique name on the host. A typical name is
org.xen.xcp.xenops.classic org.xen.xcp.xenops.xenlight A higher-level tool, such as xapi will associate VMs with individual Xenopsd names.
Running multiple Xenopsds is necessary because
The virtual hardware supported by different technologies (libxc, libxl, qemu) is expected to be different. We can guarantee the virtual hardware is stable across a rolling upgrade by running the VM on the old Xenopsd. We can then switch Xenopsds later over a VM reboot when the VM admin is happy with it. If the VM admin is unhappy then we can reboot back to the original Xenopsd again. The suspend/resume/migrate image formats will differ across technologies (again libxc vs libxl) and it will be more reliable to avoid switching technology over a migrate. In the future different security domains may have different Xenopsd instances providing even stronger isolation guarantees between domains than is possible today. Communication with Xenopsd is handled through a Xapi-global library: xcp-idl. This library supports
message framing: by default using HTTP but a binary framing format is available message encoding: by default we use JSON but XML is also available RPCs over Unix domain sockets and persistent queues. This library allows the communication details to be changed without having to change all the Xapi clients and servers.
Xenopsd has a number of “backends” which perform the low-level VM operations such as (on Xen) “create domain” “hotplug disk” “destroy domain”. These backends contain all the hypervisor-specific code including
connecting to Xenstore opening the libxc /proc/xen/privcmd interface initialising libxl contexts The following diagram shows the internal structure of Xenopsd:
At the top of the diagram two client RPC have been sent: one to start a VM and the other to fetch the latest events. The RPCs are all defined in xcp-idl/xen/xenops_interface.ml. The RPCs are received by the Xenops_server module and decomposed into “micro-ops” (labelled “μ op”). These micro ops represent actions like
create a Xen domain (recall a Xen domain is an empty shell with no memory) build a Xen domain: this is where the kernel or hvmloader is copied in launch a device model: this is where a qemu instance is started (if one is required) hotplug a device: this involves writing the frontend and backend trees to Xenstore unpause a domain (recall a Xen domain is created in the paused state) Each of these micro-ops is represented by a function call in a “backend plugin” interface. The micro-ops are enqueued in queues, one queue per VM. There is a thread pool (whose size can be changed dynamically by the admin) which pulls micro-ops from the VM queues and calls the corresponding backend function.
The active backend (there can only be one backend per Xenopsd instance) executes the micro-ops. The Xenops_server_xen backend in the picture above talks to libxc, libxl and qemu to create and destroy domains. The backend also talks to other Xapi services, in particular
it registers datasources with xcp-rrdd, telling xcp-rrdd to measure I/O throughput and vCPU utilisation it reserves memory for new domains by talking to squeezed it makes disks available by calling SMAPIv2 VDI.{at,de}tach, VDI.{,de}activate it launches subprocesses by talking to forkexecd (avoiding problems with accidental fd capture) Xenopsd backends are also responsible for monitoring running VMs. In the Xenops_server_xen backend this is done by watching Xenstore for
@releaseDomain watch events device hotplug status changes When such an event happens (for example: @releaseDomain sent when a domain requests a reboot) the corresponding operation does not happen inline. Instead the event is rebroadcast upwards to Xenops_server as a signal (for example: “VM id needs some attention”) and a “VM_stat” micro-op is queued in the appropriate queue. Xenopsd does not allow operations to run on the same VM in parallel and enforces this by:
pushing all operations pertaining to a VM to the same queue associating each VM queue to at-most-one worker pool thread The event takes the form “VM id needs some attention” and not “VM id needs to be rebooted” because, by the time the queue is flushed, the VM may well now be in a different state. Perhaps rather than being rebooted it now needs to be shutdown; or perhaps the domain is now in a good state because the reboot has already happened. The signals sent by the backend to the Xenops_server are a bit like event channel notifications in the Xen ring protocols: they are requests to ask someone to perform work, they don’t themselves describe the work that needs to be done.
An implication of this design is that it should always be possible to answer the question, “what operation should be performed to get the VM into a valid state?”. If an operation is cancelled half-way through or if Xenopsd is suddenly restarted, it will ask the question about all the VMs and perform the necessary operations. The operations must be designed carefully to make this work. For example if Xenopsd is restarted half-way through starting a VM, it must be obvious on restart that the VM should either be forcibly shutdown or rebooted to make it a valid state again. Note: we don’t demand that operations are performed as transactions; we only demand that the state they leave the system be “sensible” in the sense that the admin will recognise it and be able to continue their work.
Sometimes this can be achieved through careful ordering of side-effects within the operations, taking advantage of artifacts of the system such as:
a domain which has not been fully created will have total vCPU time = 0 and will be paused. If we see one of these we should reboot it because it may not be fully intact. In the absense of “tells” from the system, operations are expected to journal their intentions and support restart after failure.
There are three categories of metadata associated with VMs:
system metadata: this is created as a side-effect of starting VMs. This includes all the information about active disks and nics stored in Xenstore and the list of running domains according to Xen. VM: this is the configuration to use when the VM is started or rebooted. This is like a “config file” for the VM. VmExtra: this is the runtime configuration of the VM. When VM configuration is changed it often cannot be applied immediately; instead the VM continues to run with the previous configuration. We need to track the runtime configuration of the VM in order for suspend/resume and migrate to work. It is also useful to be able to tell a client, “on next reboot this value will be x but currently it is x-1”. VM and VmExtra metadata is stored by Xenopsd in the domain 0 filesystem, in a simple directory hierarchy.
`,description:"",tags:null,title:"Architecture",uri:"/new-docs/xenopsd/architecture/index.html"},{content:"",description:"",tags:null,title:"Categories",uri:"/new-docs/categories/index.html"},{content:"",description:"",tags:null,title:"Design",uri:"/new-docs/xenopsd/design/index.html"},{content:`The HA feature will restart VMs after hosts have failed, but what happens if a whole site (e.g. datacenter) is lost? A disaster recovery configuration is shown in the following diagram:
We rely on the storage array’s built-in mirroring to replicate (synchronously or asynchronously: the admin’s choice) between the primary and the secondary site. When DR is enabled the VM disk data and VM metadata are written to the storage server and mirrored. The secondary site contains the other side of the data mirror and a set of hosts, which may be powered off.
In normal operation, the DR feature allows a “dry-run” recovery where a host on the secondary site checks that it can indeed see all the VM disk data and metadata. This should be done regularly, so that admins are familiar with the process.
After a disaster, the admin breaks the mirror on the secondary site and triggers a remote power-on of the offline hosts (either using an out-of-band tool or the built-in host power-on feature of xapi). The pool master on the secondary site can connect to the storage and extract all the VM metadata. Finally the VMs can all be restarted.
When the primary site is fully recovered, the mirror can be re-synchronised and the VMs can be moved back.
`,description:"",tags:null,title:"Disaster Recovery",uri:"/new-docs/toolstack/features/DR/index.html"},{content:` ids rather than data; inherently coalescable blocking poll + async operations implies a client needs 2 connections coarse granularity similarity and differences with: XenAPI, event channels, xenstore watches https://github.com/xapi-project/xen-api/blob/30cc9a72e8726d1e7501cd01ddb27ced6d53b9be/ocaml/xapi/xapi_xenops.ml#L1467
`,description:"",tags:null,title:"Events",uri:"/new-docs/xenopsd/design/Events/index.html"},{content:"General Pluggable backends including xc: drives Xen via libxc and xenguest simulator: simulates operations for component-testing Supports running multiple instances and backends on the same host, looking after different sets of VMs Extensive configuration via command-line (see manpage) and config file Command-line tool for easy VM administration and troubleshooting User-settable degree of concurrency to get VMs started quickly VMs VM start/shutdown/reboot VM suspend/resume/checkpoint/migrate VM pause/unpause VM s3suspend/s3resume customisable SMBIOS tables for OEM-locked VMs hooks for 3rd party extensions: pre-start pre-destroy post-destroy pre-reboot per-VM xenguest replacement suppression of VM reboot loops live vCPU hotplug and unplug vCPU to pCPU affinity setting vCPU QoS settings (weight and cap for the Xen credit2 scheduler) DMC memory-ballooning support support for storage driver domains live update of VM shadow memory guest-initiated disk/nic hotunplug guest-initiated disk eject force disk/nic unplug support for ‘surprise-removable’ devices disk QoS configuration nic QoS configuration persistent RTC two-way guest agent communication for monitoring and control network carrier configuration port-locking for nics text and VNC consoles over TCP and Unix domain sockets PV kernel and ramdisk whitelisting configurable VM videoram programmable action-after-crash behaviour including: shutting down the VM, taking a crash dump or leaving the domain paused for inspection ability to move nics between bridges/switches advertises the VM memory footprints PCI passthrough support for discrete emulators (e.g. ‘demu’) PV keyboard and mouse qemu stub domains cirrus and stdvga graphics cards HVM serial console (useful for debugging) support for vGPU workaround for ‘spurious page faults’ kernel bug workaround for ‘machine address size’ kernel bug Hosts CPUid masking for heterogenous pools: reports true features and current features Host console reading Hypervisor version and capabilities reporting Host CPU querying APIs versioned json-rpc API with feature advertisements clients can disconnect, reconnect and easily resync with the latest VM state without losing updates all operations have task control including asychronous cancellation: for both subprocesses and xenstore watches progress updates subtasks per-task debug logs asynchronous event watching API advertises VM metrics memory usage balloon driver co-operativeness shadow memory usage domain ids channel passing (via sendmsg(2)) for efficient memory image copying ",description:"",tags:null,title:"Features",uri:"/new-docs/xenopsd/features/index.html"},{content:`High-Availability (HA) tries to keep VMs running, even when there are hardware failures in the resource pool, when the admin is not present. Without HA the following may happen:
during the night someone spills a cup of coffee over an FC switch; then VMs running on the affected hosts will lose access to their storage; then business-critical services will go down; then monitoring software will send a text message to an off-duty admin; then the admin will travel to the office and fix the problem by restarting the VMs elsewhere. With HA the following will happen:
during the night someone spills a cup of coffee over an FC switch; then VMs running on the affected hosts will lose access to their storage; then business-critical services will go down; then the HA software will determine which hosts are affected and shut them down; then the HA software will restart the VMs on unaffected hosts; then services are restored; then on the next working day the admin can arrange for the faulty switch to be replaced. HA is designed to handle an emergency and allow the admin time to fix failures properly.
Example The following diagram shows an HA-enabled pool, before and after a network link between two hosts fails.
When HA is enabled, all hosts in the pool
exchange periodic heartbeat messages over the network send heartbeats to a shared storage device. attempt to acquire a “master lock” on the shared storage. HA is designed to recover as much as possible of the pool after a single failure i.e. it removes single points of failure. When some subset of the pool suffers a failure then the remaining pool members
figure out whether they are in the largest fully-connected set (the “liveset”); if they are not in the largest set then they “fence” themselves (i.e. force reboot via the hypervisor watchdog) elect a master using the “master lock” restart all lost VMs. After HA has recovered a pool, it is important that the original failure is addressed because the remaining pool members may not be able to cope with any more failures.
Design HA must never violate the following safety rules:
there must be at most one master at all times. This is because the master holds the VM and disk locks. there must be at most one instance of a particular VM at all times. This is because starting the same VM twice will result in severe filesystem corruption. However to be useful HA must:
detect failures quickly; minimise the number of false-positives in the failure detector; and make the failure handling logic as robust as possible. The implementation difficulty arises when trying to be both useful and safe at the same time.
Terminology We use the following terminology:
fencing: also known as I/O fencing, refers to the act of isolating a host from network and storage. Once a host has been fenced, any VMs running there cannot generate side-effects observable to a third party. This means it is safe to restart the running VMs on another node without violating the safety-rule and running the same VM simultaneously in two locations. heartbeating: exchanging status updates with other hosts at regular pre-arranged intervals. Heartbeat messages reveal that hosts are alive and that I/O paths are working. statefile: a shared disk (also known as a “quorum disk”) on the “Heartbeat” SR which is mapped as a block device into every host’s domain 0. The shared disk acts both as a channel for heartbeat messages and also as a building block of a Pool master lock, to prevent multiple hosts becoming masters in violation of the safety-rule (a dangerous situation also known as “split-brain”). management network: the network over which the XenAPI XML/RPC requests flow and also used to send heartbeat messages. liveset: a per-Host view containing a subset of the Hosts in the Pool which are considered by that Host to be alive i.e. responding to XenAPI commands and running the VMs marked as resident_on there. When a Host b leaves the liveset as seen by Host a it is safe for Host a to assume that Host b has been fenced and to take recovery actions (e.g. restarting VMs), without violating either of the safety-rules. properly shared SR: an SR which has field shared=true; and which has a PBD connecting it to every enabled Host in the Pool; and where each of these PBDs has field currently_attached set to true. A VM whose disks are in a properly shared SR could be restarted on any enabled Host, memory and network permitting. properly shared Network: a Network which has a PIF connecting it to every enabled Host in the Pool; and where each of these PIFs has field currently_attached set to true. A VM whose VIFs connect to properly shared Networks could be restarted on any enabled Host, memory and storage permitting. agile: a VM is said to be agile if all disks are in properly shared SRs and all network interfaces connect to properly shared Networks. unprotected: an unprotected VM has field ha_always_run set to false and will never be restarted automatically on failure or have reconfiguration actions blocked by the HA overcommit protection. best-effort: a best-effort VM has fields ha_always_run set to true and ha_restart_priority set to best-effort. A best-effort VM will only be restarted if (i) the failure is directly observed; and (ii) capacity exists for an immediate restart. No more than one restart attempt will ever be made. protected: a VM is said to be protected if it will be restarted by HA i.e. has field ha_always_run set to true and field ha_restart_priority not set to \`best-effort. survival rule 1: describes the situation where hosts survive because they are in the largest network partition with statefile access. This is the normal state of the xhad daemon. survival rule 2: describes the situation where all hosts have lost access to the statefile but remain alive while they can all see each-other on the network. In this state any further failure will cause all nodes to self-fence. This state is intended to cope with the system-wide temporary loss of the storage service underlying the statefile. Assumptions We assume:
All I/O used for monitoring the health of hosts (i.e. both storage and network-based heartbeating) is along redundant paths, so that it survives a single hardware failure (e.g. a broken switch or an accidentally-unplugged cable). It is up to the admin to ensure their environment is setup correctly. The hypervisor watchdog mechanism will be able to guarantee the isolation of nodes, once communication has been lost, within a pre-arranged time period. Therefore no active power fencing equipment is required. VMs may only be marked as protected if they are fully agile i.e. able to run on any host, memory permitting. No additional constraints of any kind may be specified e.g. it is not possible to make “CPU reservations”. Pools are assumed to be homogenous with respect to CPU type and presence of VT/SVM support (also known as “HVM”). If a Pool is created with non-homogenous hosts using the --force flag then the additional constraints will not be noticed by the VM failover planner resulting in runtime failures while trying to execute the failover plans. No attempt will ever be made to shutdown or suspend “lower” priority VMs to guarantee the survival of “higher” priority VMs. Once HA is enabled it is not possible to reconfigure the management network or the SR used for storage heartbeating. VMs marked as protected are considered to have failed if they are offline i.e. the VM failure handling code is level-sensitive rather than edge-sensitive. VMs marked as best-effort are considered to have failed only when the host where they are resident is declared offline i.e. the best-effort VM failure handling code is edge-sensitive rather than level-sensitive. A single restart attempt is attempted and if this fails no further start is attempted. HA can only be enabled if all Pool hosts are online and actively responding to requests. when HA is enabled the database is configured to write all updates to the “Heartbeat” SR, guaranteeing that VM configuration changes are not lost when a host fails. Components The implementation is split across the following components:
xhad: the cluster membership daemon maintains a quorum of hosts through network and storage heartbeats xapi: used to configure the HA policy i.e. which network and storage to use for heartbeating and which VMs to restart after a failure. xen: the Xen watchdog is used to reliably fence the host when the host has been (partially or totally) isolated from the cluster To avoid a “split-brain”, the cluster membership daemon must “fence” (i.e. isolate) nodes when they are not part of the cluster. In general there are 2 approaches:
cut the power of remote hosts which you can’t talk to on the network any more. This is the approach taken by most open-source clustering software since it is simpler. However it has the downside of requiring the customer buy more hardware and set it up correctly. rely on the remote hosts using a watchdog to cut their own power (i.e. halt or reboot) after a timeout. This relies on the watchdog being reliable. Most other people don’t trust the Linux watchdog; after all the Linux kernel is highly threaded, performs a lot of (useful) functions and kernel bugs which result in deadlocks do happen. We use the Xen watchdog because we believe that the Xen hypervisor is simple enough to reliably fence the host (via triggering a reboot of domain 0 which then triggers a host reboot). xhad xhad is the cluster membership daemon: it exchanges heartbeats with the other nodes to determine which nodes are still in the cluster (the “live set”) and which nodes have definitely failed (through watchdog fencing). When a host has definitely failed, xapi will unlock all the disks and restart the VMs according to the HA policy.
Since Xapi is a critical part of the system, the xhad also acts as a Xapi watchdog. It polls Xapi every few seconds and checks if Xapi can respond. If Xapi seems to have failed then xhad will restart it. If restarts continue to fail then xhad will consider the host to have failed and self-fence.
xhad is configured via a simple config file written on each host in /etc/xensource/xhad.conf. The file must be identical on each host in the cluster. To make changes to the file, HA must be disabled and then re-enabled afterwards. Note it may not be possible to re-enable HA depending on the configuration change (e.g. if a host has been added but that host has a broken network configuration then this will block HA enable).
The xhad.conf file is written in XML and contains
pool-wide configuration: this includes a list of all hosts which should be in the liveset and global timeout information local host configuration: this identifies the local host and described which local network interface and block device to use for heartbeating. The following is an example xhad.conf file:
<?xml version="1.0" encoding="utf-8"?> <xhad-config version="1.0"> <!--pool-wide configuration--> <common-config> <GenerationUUID>xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx</GenerationUUID> <UDPport>694</UDPport> <!--for each host, specify host UUID, and IP address--> <host> <HostID>xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx</HostID> <IPaddress>xxx.xxx.xxx.xx1</IPaddress> </host> <host> <HostID>xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx</HostID> <IPaddress>xxx.xxx.xxx.xx2</IPaddress> </host> <host> <HostID>xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx</HostID> <IPaddress>xxx.xxx.xxx.xx3</IPaddress> </host> <!--optional parameters [sec] --> <parameters> <HeartbeatInterval>4</HeartbeatInterval> <HeartbeatTimeout>30</HeartbeatTimeout> <StateFileInterval>4</StateFileInterval> <StateFileTimeout>30</StateFileTimeout> <HeartbeatWatchdogTimeout>30</HeartbeatWatchdogTimeout> <StateFileWatchdogTimeout>45</StateFileWatchdogTimeout> <BootJoinTimeout>90</BootJoinTimeout> <EnableJoinTimeout>90</EnableJoinTimeout> <XapiHealthCheckInterval>60</XapiHealthCheckInterval> <XapiHealthCheckTimeout>10</XapiHealthCheckTimeout> <XapiRestartAttempts>1</XapiRestartAttempts> <XapiRestartTimeout>30</XapiRestartTimeout> <XapiLicenseCheckTimeout>30</XapiLicenseCheckTimeout> </parameters> </common-config> <!--local host configuration--> <local-config> <localhost> <HostID>xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx2</HostID> <HeartbeatInterface> xapi1</HeartbeatInterface> <HeartbeatPhysicalInterface>bond0</HeartbeatPhysicalInterface> <StateFile>/dev/statefiledevicename</StateFile> </localhost> </local-config> </xhad-config>The fields have the following meaning:
GenerationUUID: a UUID generated each time HA is reconfigured. This allows xhad to tell an old host which failed; had been removed from the configuration; repaired and then restarted that the world has changed while it was away. UDPport: the port number to use for network heartbeats. It’s important to allow this traffic through the firewall and to make sure the same port number is free on all hosts (beware of portmap services occasionally binding to it). HostID: a UUID identifying a host in the pool. We would normally use xapi’s notion of a host uuid. IPaddress: any IP address on the remote host. We would normally use xapi’s notion of a management network. HeartbeatTimeout: if a heartbeat packet is not received for this many seconds, then xhad considers the heartbeat to have failed. This is the user-supplied “HA timeout” value, represented below as T. T must be bigger than 10; we would normally use 60s. StateFileTimeout: if a storage update is not seen for a host for this many seconds, then xhad considers the storage heartbeat to have failed. We would normally use the same value as the HeartbeatTimeout T. HeartbeatInterval: interval between heartbeat packets sent. We would normally use a value 2 <= t <= 6, derived from the user-supplied HA timeout via t = (T + 10) / 10 StateFileInterval: interval betwen storage updates (also known as “statefile updates”). This would normally be set to the same value as HeartbeatInterval. HeartbeatWatchdogTimeout: If the host does not send a heartbeat for this amount of time then the host self-fences via the Xen watchdog. We normally set this to T. StateFileWatchdogTimeout: If the host does not update the statefile for this amount of time then the host self-fences via the Xen watchdog. We normally set this to T+15. BootJoinTimeout: When the host is booting and joining the liveset (i.e. the cluster), consider the join a failure if it takes longer than this amount of time. We would normally set this to T+60. EnableJoinTimeout: When the host is enabling HA for the first time, consider the enable a failure if it takes longer than this amount of time. We would normally set this to T+60. XapiHealthCheckInterval: Interval between “health checks” where we run a script to check whether Xapi is responding or not. XapiHealthCheckTimeout: Number of seconds to wait before assuming that Xapi has deadlocked during a “health check”. XapiRestartAttempts: Number of Xapi restarts to attempt before concluding Xapi has permanently failed. XapiRestartTimeout: Number of seconds to wait for a Xapi restart to complete before concluding it has failed. XapiLicenseCheckTimeout: Number of seconds to wait for a Xapi license check to complete before concluding that xhad should terminate. In addition to the config file, Xhad exposes a simple control API which is exposed as scripts:
ha_set_pool_state (Init | Invalid): sets the global pool state to “Init” (before starting HA) or “Invalid” (causing all other daemons who can see the statefile to shutdown) ha_start_daemon: if the pool state is “Init” then the daemon will attempt to contact other daemons and enable HA. If the pool state is “Active” then the host will attempt to join the existing liveset. ha_query_liveset: returns the current state of the cluster. ha_propose_master: returns whether the current node has been elected pool master. ha_stop_daemon: shuts down the xhad on the local host. Note this will not disarm the Xen watchdog by itself. ha_disarm_fencing: disables fencing on the local host. ha_set_excluded: when a host is being shutdown cleanly, record the fact that the VMs have all been shutdown so that this host can be ignored in future cluster membership calculations. Fencing Xhad continuously monitors whether the host should remain alive, or if it should self-fence. There are two “survival rules” which will keep a host alive; if neither rule applies (or if xhad crashes or deadlocks) then the host will fence. The rules are:
Xapi is running; the storage heartbeats are visible; this host is a member of the “best” partition (as seen through the storage heartbeats) Xapi is running; the storage is inaccessible; all hosts which should be running (i.e. not those “excluded” by being cleanly shutdown) are online and have also lost storage access (as seen through the network heartbeats). where the “best” partition is the largest one if that is unique, or if there are multiple partitions of the same size then the one containing the lowest host uuid is considered best.
The first survival rule is the “normal” case. The second rule exists only to prevent the storage from becoming a single point of failure: all hosts can remain alive until the storage is repaired. Note that if a host has failed and has not yet been repaired, then the storage becomes a single point of failure for the degraded pool. HA removes single point of failures, but multiple failures can still cause problems. It is important to fix failures properly after HA has worked around them.
xapi Xapi is responsible for
exposing an interface for setting HA policy creating VDIs (disks) on shared storage for heartbeating and storing the pool database arranging for these disks to be attached on host boot, before the “SRmaster” is online configuring and managing the xhad heartbeating daemon The HA policy APIs include
methods to determine whether a VM is agile i.e. can be restarted in principle on any host after a failure planning for a user-specified number of host failures and enforcing access control restarting failed protected VMs in policy order The HA policy settings are stored in the Pool database which is written (synchronously) to a VDI in the same SR that’s being used for heartbeating. This ensures that the database can be recovered after a host fails and the VMs are recovered.
Xapi stores 2 settings in its local database:
ha_disable_failover_actions: this is set to false when we want nodes to be able to recover VMs – this is the normal case. It is set to true during the HA disable process to prevent a split-brain forming while HA is only partially enabled. ha_armed: this is set to true to tell Xapi to start Xhad during host startup and wait to join the liveset. Disks on shared storage The regular disk APIs for creating, destroying, attaching, detaching (etc) disks need the SRmaster (usually but not always the Pool master) to be online to allow the disks to be locked. The SRmaster cannot be brought online until the host has joined the liveset. Therefore we have a cyclic dependency: joining the liveset needs the statefile disk to be attached but attaching a disk requires being a member of the liveset already.
The dependency is broken by adding an explicit “unlocked” attach storage API called VDI_ATTACH_FROM_CONFIG. Xapi uses the VDI_GENERATE_CONFIG API during the HA enable operation and stores away the result. When the system boots the VDI_ATTACH_FROM_CONFIG is able to attach the disk without the SRmaster.
The role of Host.enabled The Host.enabled flag is used to mean, “this host is ready to start VMs and should be included in failure planning”. The VM restart planner assumes for simplicity that all protected VMs can be started anywhere; therefore all involved networks and storage must be properly shared. If a host with an unplugged PBD were to become enabled then the corresponding SR would cease to be properly shared, all the VMs would cease to be agile and the VM restart logic would fail.
To ensure the VM restart logic always works, great care is taken to make sure that Hosts may only become enabled when their networks and storage are properly configured. This is achieved by:
when the master boots and initialises its database it sets all Hosts to dead and disabled and then signals the HA background thread (signal_database_state_valid) to wake up from sleep and start processing liveset information (and potentially setting hosts to live) when a slave calls Pool.hello (i.e. after the slave has rebooted), the master sets it to disabled, allowing it a grace period to plug in its storage; when a host (master or slave) successfully plugs in its networking and storage it calls consider_enabling_host which checks that the preconditions are met and then sets the host to enabled; and when a slave notices its database connection to the master restart (i.e. after the master xapi has just restarted) it calls consider_enabling_host} The steady-state When HA is enabled and all hosts are running normally then each calls ha_query_liveset every 10s.
Slaves check to see if the host they believe is the master is alive and has the master lock. If another node has become master then the slave will rewrite its pool.conf and restart. If no node is the master then the slave will call on_master_failure, proposing itself and, if it is rejected, checking the liveset to see which node acquired the lock.
The master monitors the liveset and updates the Host_metrics.live flag of every host to reflect the liveset value. For every host which is not in the liveset (i.e. has fenced) it enumerates all resident VMs and marks them as Halted. For each protected VM which is not running, the master computes a VM restart plan and attempts to execute it. If the plan fails then a best-effort VM.start call is attempted. Finally an alert is generated if the VM could not be restarted.
Note that XenAPI heartbeats are still sent when HA is enabled, even though they are not used to drive the values of the Host_metrics.live field. Note further that, when a host is being shutdown, the host is immediately marked as dead and its host reference is added to a list used to prevent the Host_metrics.live being accidentally reset back to live again by the asynchronous liveset query. The Host reference is removed from the list when the host restarts and calls Pool.hello.
Planning and overcommit The VM failover planning code is sub-divided into two pieces, stored in separate files:
binpack.ml: contains two algorithms for packing items of different sizes (i.e. VMs) into bins of different sizes (i.e. Hosts); and xapi_ha_vm_failover.ml: interfaces between the Pool database and the binpacker; also performs counterfactual reasoning for overcommit protection. The input to the binpacking algorithms are configuration values which represent an abstract view of the Pool:
type ('a, 'b) configuration = { hosts: ('a * int64) list; (** a list of live hosts and free memory *) vms: ('b * int64) list; (** a list of VMs and their memory requirements *) placement: ('b * 'a) list; (** current VM locations *) total_hosts: int; (** total number of hosts in the pool 'n' *) num_failures: int; (** number of failures to tolerate 'r' *) }Note that:
the memory required by the VMs listed in placement has already been substracted from the total memory of the hosts; it doesn’t need to be subtracted again. the free memory of each host has already had per-host miscellaneous overheads subtracted from it, including that used by unprotected VMs, which do not appear in the VM list. the total number of hosts in the pool (total_hosts) is a constant for any particular invocation of HA. the number of failures to tolerate (num_failures) is the user-settable value from the XenAPI Pool.ha_host_failures_to_tolerate. There are two algorithms which satisfy the interface:
sig plan_always_possible: ('a, 'b) configuration -> bool; get_specific_plan: ('a, 'b) configuration -> 'b list -> ('b * 'a) list endThe function get_specific_plan takes a configuration and a list of Hosts which have failed. It returns a VM restart plan represented as a VM to Host association list. This is the function called by the background HA VM restart thread on the master.
The function plan_always_possible returns true if every sequence of Host failures of length num_failures (irrespective of whether all hosts failed at once, or in multiple separate episodes) would result in calls to get_specific_plan which would allow all protected VMs to be restarted. This function is heavily used by the overcommit protection logic as well as code in XenCenter which aims to maximise failover capacity using the counterfactual reasoning APIs:
Pool.ha_compute_max_host_failures_to_tolerate Pool.ha_compute_hypothetical_max_host_failures_to_tolerateThere are two binpacking algorithms: the more detailed but expensive algorithmm is used for smaller/less complicated pool configurations while the less detailed, cheaper algorithm is used for the rest. The choice between algorithms is based only on total_hosts (n) and num_failures (r). Note that the choice of algorithm will only change if the number of Pool hosts is varied (requiring HA to be disabled and then enabled) or if the user requests a new num_failures target to plan for.
The expensive algorithm uses an exchaustive search with a “biggest-fit-decreasing” strategy that takes the biggest VMs first and allocates them to the biggest remaining Host. The implementation keeps the VMs and Hosts as sorted lists throughout. There are a number of transformations to the input configuration which are guaranteed to preserve the existence of a VM to host allocation (even if the actual allocation is different). These transformations which are safe are:
VMs may be removed from the list VMs may have their memory requirements reduced Hosts may be added Hosts may have additional memory added. The cheaper algorithm is used for larger Pools where the state space to search is too large. It uses the same “biggest-fit-decreasing” strategy with the following simplifying approximations:
every VM that fails is as big as the biggest the number of VMs which fail due to a single Host failure is always the maximum possible (even if these are all very small VMs) the largest and most capable Hosts fail An informal argument that these approximations are safe is as follows: if the maximum number of VMs fail, each of which is size of the largest and we can find a restart plan using only the smaller hosts then any real failure:
can never result in the failure of more VMs; can never result in the failure of bigger VMs; and can never result in less host capacity remaining. Therefore we can take this almost-certainly-worse-than-worst-case failure plan and:
replace the remaining hosts in the worst case plan with the real remaining hosts, which will be the same size or larger; and replace the failed VMs in the worst case plan with the real failed VMs, which will be fewer or the same in number and smaller or the same in size. Note that this strategy will perform best when each host has the same number of VMs on it and when all VMs are approximately the same size. If one very big VM exists and a lot of smaller VMs then it will probably fail to find a plan. It is more tolerant of differing amounts of free host memory.
Overcommit protection Overcommit protection blocks operations which would prevent the Pool being able to restart protected VMs after host failure. The Pool may become unable to restart protected VMs in two general ways: (i) by running out of resource i.e. host memory; and (ii) by altering host configuration in such a way that VMs cannot be started (or the planner thinks that VMs cannot be started).
API calls which would change the amount of host memory currently in use (VM.start, VM.resume, VM.migrate etc) have been modified to call the planning functions supplying special “configuration change” parameters. Configuration change values represent the proposed operation and have type
type configuration_change = { (** existing VMs which are leaving *) old_vms_leaving: (API.ref_host * (API.ref_VM * API.vM_t)) list; (** existing VMs which are arriving *) old_vms_arriving: (API.ref_host * (API.ref_VM * API.vM_t)) list; (** hosts to pretend to disable *) hosts_to_disable: API.ref_host list; (** new number of failures to consider *) num_failures: int option; (** new VMs to restart *) new_vms_to_protect: API.ref_VM list; }A VM migration will be represented by saying the VM is “leaving” one host and “arriving” at another. A VM start or resume will be represented by saying the VM is “arriving” on a host.
Note that no attempt is made to integrate the overcommit protection with the general VM.start host chooser as this would be quite expensive.
Note that the overcommit protection calls are written as asserts called within the message forwarder in the master, holding the main forwarding lock.
API calls which would change the system configuration in such a way as to prevent the HA restart planner being able to guarantee to restart protected VMs are also blocked. These calls include:
VBD.create: where the disk is not in a properly shared SR VBD.insert: where the CDROM is local to a host VIF.create: where the network is not properly shared PIF.unplug: when the network would cease to be properly shared PBD.unplug: when the storage would cease to be properly shared Host.enable: when some network or storage would cease to be properly shared (e.g. if this host had a broken storage configuration) xen The Xen hypervisor has per-domain watchdog counters which, when enabled, decrement as time passes and can be reset from a hypercall from the domain. If the domain fails to make the hypercall and the timer reaches zero then the domain is immediately shutdown with reason reboot. We configure Xen to reboot the host when domain 0 enters this state.
High-level operations Enabling HA Before HA can be enabled the admin must take care to configure the environment properly. In particular:
NIC bonds should be available for network heartbeats; multipath should be configured for the storage heartbeats; all hosts should be online and fully-booted. The XenAPI client can request a specific shared SR to be used for storage heartbeats, otherwise Xapi will use the Pool’s default SR. Xapi will use VDI_GENERATE_CONFIG to ensure the disk will be attached automatically on system boot before the liveset has been joined.
Note that extra effort is made to re-use any existing heartbeat VDIS so that
if HA is disabled with some hosts offline, when they are rebooted they stand a higher chance of seeing a well-formed statefile with an explicit invalid state. If the VDIs were destroyed on HA disable then hosts which boot up later would fail to attach the disk and it would be harder to distinguish between a temporary storage failure and a permanent HA disable. the heartbeat SR can be created on expensive low-latency high-reliability storage and made as small as possible (to minimise infrastructure cost), safe in the knowledge that if HA enables successfully once, it won’t run out of space and fail to enable in the future. The Xapi-to-Xapi communication looks as follows:
The Xapi Pool master calls Host.ha_join_liveset on all hosts in the pool simultaneously. Each host runs the ha_start_daemon script which starts Xhad. Each Xhad starts exchanging heartbeats over the network and storage defined in the xhad.conf.
Joining a liveset The Xhad instances exchange heartbeats and decide which hosts are in the “liveset” and which have been fenced.
After joining the liveset, each host clears the “excluded” flag which would have been set if the host had been shutdown cleanly before – this is only needed when a host is shutdown cleanly and then restarted.
Xapi periodically queries the state of xhad via the ha_query_liveset command. The state will be Starting until the liveset is fully formed at which point the state will be Online.
When the ha_start_daemon script returns then Xapi will decide whether to stand for master election or not. Initially when HA is being enabled and there is a master already, this node will be expected to stand unopposed. Later when HA notices that the master host has been fenced, all remaining hosts will stand for election and one of them will be chosen.
Shutting down a host When a host is to be shutdown cleanly, it can be safely “excluded” from the pool such that a future failure of the storage heartbeat will not cause all pool hosts to self-fence (see survival rule 2 above). When a host is “excluded” all other hosts know that the host does not consider itself a master and has no resources locked i.e. no VMs are running on it. An excluded host will never allow itself to form part of a “split brain”.
Once a host has given up its master role and shutdown any VMs, it is safe to disable fencing with ha_disarm_fencing and stop xhad with ha_stop_daemon. Once the daemon has been stopped the “excluded” bit can be set in the statefile via ha_set_excluded and the host safely rebooted.
Restarting a host When a host restarts after a failure Xapi notices that ha_armed is set in the local database. Xapi
runs the attach-static-vdis script to attach the statefile and database VDIs. This can fail if the storage is inaccessible; Xapi will retry until it succeeds. runs the ha_start_daemon to join the liveset, or determine that HA has been cleanly disabled (via setting the state to Invalid). In the special case where Xhad fails to access the statefile and the host used to be a slave then Xapi will try to contact the previous master and find out
who the new master is; whether HA is enabled on the Pool or not. If Xapi can confirm that HA was disabled then it will disarm itself and join the new master. Otherwise it will keep waiting for the statefile to recover.
In the special case where the statefile has been destroyed and cannot be recovered, there is an emergency HA disable API the admin can use to assert that HA really has been disabled, and it’s not simply a connectivity problem. Obviously this API should only be used if the admin is totally sure that HA has been disabled.
Disabling HA There are 2 methods of disabling HA: one for the “normal” case when the statefile is available; and the other for the “emergency” case when the statefile has failed and can’t be recovered.
Disabling HA cleanly HA can be shutdown cleanly when the statefile is working i.e. when hosts are alive because of survival rule 1. First the master Xapi tells the local Xhad to mark the pool state as “invalid” using ha_set_pool_state. Every xhad instance will notice this state change the next time it performs a storage heartbeat. The Xhad instances will shutdown and Xapi will notice that HA has been disabled the next time it attempts to query the liveset.
If a host loses access to the statefile (or if none of the hosts have access to the statefile) then HA can be disabled uncleanly.
Disabling HA uncleanly The Xapi master first calls Host.ha_disable_failover_actions on each host which sets ha_disable_failover_decisions in the lcoal database. This prevents the node rebooting, gaining statefile access, acquiring the master lock and restarting VMs when other hosts have disabled their fencing (i.e. a “split brain”).
Once the master is sure that no host will suddenly start recovering VMs it is safe to call Host.ha_disarm_fencing which runs the script ha_disarm_fencing and then shuts down the Xhad with ha_stop_daemon.
Add a host to the pool We assume that adding a host to the pool is an operation the admin will perform manually, so it is acceptable to disable HA for the duration and to re-enable it afterwards. If a failure happens during this operation then the admin will take care of it by hand.
`,description:"",tags:null,title:"High-Availability",uri:"/new-docs/toolstack/features/HA/index.html"},{content:`There are a number of hook points at which xenopsd may execute certain scripts. These scripts are found in hook-specific directories of the form /etc/xapi.d/<hookname>/. All executable scripts in these directories are run with the following arguments:
<script.sh> -reason <reason> -vmuuid <uuid of VM> The scripts are executed in filename-order. By convention, the filenames are usually of the form 10resetvdis.
The hook points are:
vm-pre-shutdown vm-pre-migrate vm-post-migrate (Dundee only) vm-pre-start vm-pre-reboot vm-pre-resume vm-post-resume (Dundee only) vm-post-destroy and the reason codes are:
clean-shutdown hard-shutdown clean-reboot hard-reboot suspend source -- passed to pre-migrate hook on source host destination -- passed to post-migrate hook on destination (Dundee only) none For example, in order to execute a script on VM shutdown, it would be sufficient to create the script in the post-destroy hook point:
/etc/xapi.d/vm-post-destroy/01myscript.sh containing
#!/bin/bash echo I was passed $@ > /tmp/output And when, for example, VM e30d0050-8f15-e10d-7613-cb2d045c8505 is shut-down, the script is executed:
[vagrant@localhost ~]$ sudo xe vm-shutdown --force uuid=e30d0050-8f15-e10d-7613-cb2d045c8505 [vagrant@localhost ~]$ cat /tmp/output I was passed -vmuuid e30d0050-8f15-e10d-7613-cb2d045c8505 -reason hard-shutdown `,description:"",tags:null,title:"Hooks",uri:"/new-docs/xenopsd/design/hooks/index.html"},{content:`Memory is used for many things:
the hypervisor code: this is the Xen executable itself the hypervisor heap: this is needed for per-domain structures and per-vCPU structures the crash kernel: this is needed to collect information after a host crash domain RAM: this is the memory the VM believes it has shadow memory: for HVM guests running on hosts without hardware assisted paging (HAP) Xen uses shadow to optimise page table updates. For all guests shadow is used during live migration for tracking the memory transfer. video RAM for the virtual graphics card Some of these are constants (e.g. hypervisor code) while some depend on the VM configuration (e.g. domain RAM). Xapi calls the constants “host overhead” and the variables due to VM configuration as “VM overhead”. There is no low-level API to query this information, therefore xapi will sample the host overheads at system boot time and model the per-VM overheads.
Host overhead The host overhead is not managed by xapi, instead it is sampled. After the host boots and before any VMs start, xapi asks Xen how much memory the host has in total, and how much memory is currently free. Xapi subtracts the free from the total and stores this as the host overhead.
VM overhead The inputs to the model are
VM.memory_static_max: the maximum amount of RAM the domain will be able to use VM.HVM_shadow_multiplier: allows the shadow memory to be increased VM.VCPUs_max: the maximum number of vCPUs the domain will be able to use First the shadow memory is calculated, in MiB
Second the VM overhead is calculated, in MiB
Memory required to start a VM If ballooning is disabled, the memory required to start a VM is the same as the VM overhead above.
If ballooning is enabled then the memory calculation above is modified to use the VM.memory_dynamic_max rather than the VM.memory_static_max.
Memory required to migrate a VM If ballooning is disabled, the memory required to receive a migrating VM is the same as the VM overhead above.
If ballooning is enabled, then the VM will first be ballooned down to VM.memory_dynamic_min and then it will be migrated across. If the VM fails to balloon all the way down, then correspondingly more memory will be required on the receiving side.
`,description:"",tags:null,title:"Host memory accounting",uri:"/new-docs/xapi/memory/index.html"},{content:"",description:"",tags:null,title:"How to add....",uri:"/new-docs/xapi/guides/howtos/index.html"},{content:`Communication between the Toolstack daemon is built upon libraries from a component called xapi-idl.
Abstracts communication between daemons over the message-switch using JSON/RPC. Contains the definition of the interfaces exposed by the daemons (except xapi). `,description:"",tags:null,title:"Interfaces",uri:"/new-docs/toolstack/high-level/interfaces/index.html"},{content:" sequenceDiagram autonumber participant tx as sender participant rx0 as receiver thread 0 participant rx1 as receiver thread 1 participant rx2 as receiver thread 2 activate tx tx->>rx0: VM.import_metadata tx->>tx: Squash memory to dynamic-min tx->>rx1: HTTP /migrate/vm activate rx1 rx1->>rx1: VM_receive_memory<br/>VM_create (00000001)<br/>VM_restore_vifs rx1->>tx: handshake (control channel)<br/>Synchronisation point 1 tx->>rx2: HTTP /migrate/mem activate rx2 rx2->>tx: handshake (memory channel)<br/>Synchronisation point 1-mem tx->>rx1: handshake (control channel)<br/>Synchronisation point 1-mem ACK rx2->>rx1: memory fd tx->>rx1: VM_save/VM_restore<br/>Synchronisation point 2 tx->>tx: VM_rename rx1->>rx2: exit deactivate rx2 tx->>rx1: handshake (control channel)<br/>Synchronisation point 3 rx1->>rx1: VM_rename<br/>VM_restore_devices<br/>VM_unpause<br/>VM_set_domain_action_request rx1->>tx: handshake (control channel)<br/>Synchronisation point 4 deactivate rx1 tx->>tx: VM_shutdown<br/>VM_remove deactivate tx ",description:"",tags:null,title:"Live Migration Sequence Diagram",uri:"/new-docs/xenopsd/walkthroughs/live-migration/index.html"},{content:`Let’s trace through interesting operations to see how the whole system works.
Starting a VM Migrating a VM Shutting down a VM and waiting for it to happen A VM wants to reboot itself A disk is hotplugged A disk refuses to hotunplug A VM is suspended `,description:"",tags:null,title:"Operation Walk-Throughs",uri:"/new-docs/xenopsd/walkthroughs/index.html"},{content:`We are currently (Dec 2013) undergoing a transition from the ‘classic’ xenopsd backend (built upon calls to libxc) to the ‘xenlight’ backend built on top of the officially supported libxl API.
During this work, we have come across an incompatibility between the suspend images created using the ‘classic’ backend and those created using the new libxl-based backend. This needed to be fixed to enable RPU to any new version of XenServer.
Historic ‘classic’ stack Prior to this work, xenopsd was involved in the construction of the suspend image and we ended up with an image with the following format:
+-----------------------------+ | "XenSavedDomain\\n" | <-- added by xenopsd-classic |-----------------------------| | Memory image dump | <-- libxc |-----------------------------| | "QemuDeviceModelRecord\\n" | | <size of following record> | <-- added by xenopsd-classic | (a 32-bit big-endian int) | |-----------------------------| | "QEVM" | <-- libxc/qemu | Qemu device record | +-----------------------------+ We have also been carrying a patch in the Xen patchqueue against xc_domain_restore. This patch (revert_qemu_tail.patch) stopped xc_domain_restore from attempting to read past the memory image dump. At which point xenopsd-classic would just take over and restore what it had put there.
Requirements for new stack For xenopsd-xenlight to work, we need to operate without the revert_qemu_tail.patch since libxl assumes it is operating on top of an upstream libxc.
We need the following relationship between suspend images created on one backend being able to be restored on another backend. Where the backends are old-classic (OC), new-classic (NC) and xenlight (XL). Obviously all suspend images created on any backend must be able to be restored on the same backend:
OC _______ NC _______ XL \\ >>>>> >>>>> / \\__________________/ >>>>>>>>>>>>>>>> It turns out this was not so simple. After removing the patch against xc_domain_restore and allowing libxc to restore the hvm_buffer_tail, we found that supsend images created with OC (detailed in the previous section) are not of a valid format for two reasons:
i. The "XenSavedDomain\\n" was extraneous; ii. The Qemu signature section (prior to the record) is not of valid form.
It turns out that the section with the Qemu signature can be one of the following:
a. "QemuDeviceModelRecord" (NB. no newline) followed by the record to EOF; b. "DeviceModelRecord0002" then a uint32_t length followed by record; c. "RemusDeviceModelState" then a uint32_t length followed by record; The old-classic (OC) backend not only uses an invalid signature (since it contains a trailing newline) but it also includes a length, and the length is in big-endian when the uint32_t is seen to be little-endian.
We considered creating a proxy for the fd in the incompatible cases but since this would need to be a 22-lookahead byte-by-byte proxy this was deemed impracticle. Instead we have made patched libxc with a much simpler patch to understand this legacy format.
Because peek-ahead is not possible on pipes, the patch for (ii) needed to be applied at a point where the hvm tail had been read completely. We piggy-backed on the point after (a) had been detected. At this point the remainder of the fd is buffered (only around 7k) and the magic “QEVM” is expected at the head of this buffer. So we simply added a patch to check if there was a pesky newline and the buffer[5:8] was “QEVM” and if it was we could discard the first 5 bytes:
0 1 2 3 4 5 6 7 8 Legacy format from OC: [...| \\n | \\x | \\x | \\x | \\x | Q | E | V | M |...] Required at this point: [...| Q | E | V | M |...] Changes made To make the above use-cases work, we have made the following changes:
1. Make new-classic (NC) not restore Qemu tail (let libxc do it) xenopsd.git:ef3bf4b 2. Make new-classic use valid signature (b) for future restore images xenopsd.git:9ccef3e 3. Make xc_domain_restore in libxc understand legacy xenopsd (OC) format xen-4.3.pq.hg:libxc-restore-legacy-image.patch 4. Remove revert-qemu-tail.patch from Xen patchqueue xen-4.3.pq.hg:3f0e16f2141e 5. Make xenlight (XL) use "XenSavedDomain\\n" start-of-image signature xenopsd.git:dcda545 This has made the required use-cases work as follows:
OC __134__ NC __245__ XL \\ >>>>> >>>>> / \\_______345________/ >>>>>>>>>>>>>>>> And the suspend-resume on same backends work by virtue of:
OC --> OC : Just works NC --> NC : By 1,2,4 XL --> XL : By 4 (5 is used but not required) New components The output of the changes above are:
A new xenops-xc binary for NC A new xenops-xl binary for XL A new libxenguest.4.3 for both of NC and XL Future considerations This should serve as a useful reference when considering making changes to the suspend image in any way.
`,description:"",tags:null,title:"Requirements for suspend image framing",uri:"/new-docs/xenopsd/design/suspend-image-considerations/index.html"},{content:`Snapshots represent the state of a VM, or a disk (VDI) at a point in time. They can be used for:
backups (hourly, daily, weekly etc) experiments (take snapshot, try something, revert back again) golden images (install OS, get it just right, clone it 1000s of times) Read more about the Snapshot APIs.
Disk snapshots Disks are represented in the XenAPI as VDI objects. Disk snapshots are represented as VDI objects with the flag is_a_snapshot set to true. Snapshots are always considered read-only, and should only be used for backup or cloning into new disks. Disk snapshots have a lifetime independent of the disk they are a snapshot of i.e. if someone deletes the original disk, the snapshots remain. This contrasts with some storage arrays in which snapshots are “second class” objects which are automatically deleted when the original disk is deleted.
Disks are implemented in Xapi via “Storage Manager” (SM) plugins. The SM plugins conform to an api (the SMAPI) which has operations including
vdi_create: make a fresh disk, full of zeroes vdi_snapshot: create a snapshot of a disk File-based vhd implementation The existing “EXT” and “NFS” file-based Xapi SM plugins store disk data in trees of .vhd files as in the following diagram:
From the XenAPI point of view, we have one current VDI and a set of snapshots, each taken at a different point in time. These VDIs correspond to leaf vhds in a tree stored on disk, where the non-leaf nodes contain all the shared blocks.
The vhd files are always thinly-provisioned which means they only allocate new blocks on an as-needed basis. The snapshot leaf vhd files only contain vhd metadata and therefore are very small (a few KiB). The parent nodes containing the shared blocks only contain the shared blocks. The current leaf initially contains only the vhd metadata and therefore is very small (a few KiB) and will only grow when the VM writes blocks.
File-based vhd implementations are a good choice if a “gold image” snapshot is going to be cloned lots of times.
Block-based vhd implementation The existing “LVM”, “LVMoISCSI” and “LVMoHBA” block-based Xapi SM plugins store disk data in trees of .vhd files contained within LVM logical volumes:
Non-snapshot VDIs are always stored full size (a.k.a. thickly-provisioned). When parent nodes are created they are automatically shrunk to the minimum size needed to store the shared blocks. The LVs corresponding with snapshot VDIs only contain vhd metadata and by default consume 8MiB. Note: this is different to VDI.clones which are stored full size.
Block-based vhd implementations are not a good choice if a “gold image” snapshot is going to be cloned lots of times, since each clone will be stored full size.
Hypothetical LUN implementation A hypothetical Xapi SM plugin could use LUNs on an iSCSI storage array as VDIs, and the array’s custom control interface to implement the “snapshot” operation:
From the XenAPI point of view, we have one current VDI and a set of snapshots, each taken at a different point in time. These VDIs correspond to LUNs on the same iSCSI target, and internally within the target these LUNs are comprised of blocks from a large shared copy-on-write pool with support for dedup.
Reverting disk snapshots There is no current way to revert in-place a disk to a snapshot, but it is possible to create a writable disk by “cloning” a snapshot.
VM snapshots Let’s say we have a VM, “VM1” that has 2 disks. Concentrating only on the VM, VBDs and VDIs, we have the following structure:
When we take a snapshot, we first ask the storage backends to snapshot all of the VDIs associated with the VM, producing new VDI objects. Then we copy all of the metadata, producing a new ‘snapshot’ VM object, complete with its own VBDs copied from the original, but now pointing at the snapshot VDIs. We also copy the VIFs and VGPUs but for now we will ignore those.
This process leads to a set of objects that look like this:
We have fields that help navigate the new objects: VM.snapshot_of, and VDI.snapshot_of. These, like you would expect, point to the relevant other objects.
Deleting VM snapshots When a snapshot is deleted Xapi calls the SM API vdi_delete. The Xapi SM plugins which use vhd format data do not reclaim space immediately; instead they mark the corresponding vhd leaf node as “hidden” and, at some point later, run a garbage collector process.
The garbage collector will first determine whether a “coalesce” should happen i.e. whether any parent nodes have only one child i.e. the “shared” blocks are only “shared” with one other node. In the following example the snapshot delete leaves such a parent node and the coalesce process copies blocks from the redundant parent’s only child into the parent:
Note that if the vhd data is being stored in LVM, then the parent node will have had to be expanded to full size to accommodate the writes. Unfortunately this means the act of reclaiming space actually consumes space itself, which means it is important to never completely run out of space in such an SR.
Once the blocks have been copied, we can now cut one of the parents out of the tree by relinking its children into their grandparent:
Finally the garbage collector can remove unused vhd files / LVM LVs:
Reverting VM snapshots The XenAPI call VM.revert overwrites the VM metadata with the snapshot VM metadata, deletes the current VDIs and replaces them with clones of the snapshot VDIs. Note there is no “vdi_revert” in the SMAPI.
Revert implementation details This is the process by which we revert a VM to a snapshot. The first thing to notice is that there is some logic that is called from message_forwarding.ml, which uses some low-level database magic to turn the current VM record into one that looks like the snapshot object. We then go to the rest of the implementation in xapi_vm_snapshot.ml. First, we shut down the VM if it is currently running. Then, we revert all of the VBDs, VIFs and VGPUs. To revert the VBDs, we need to deal with the VDIs underneath them. In order to create space, the first thing we do is delete all of the VDIs currently attached via VBDs to the VM. We then clone the disks from the snapshot. Note that there is no SMAPI operation ‘revert’ currently - we simply clone from the snapshot VDI. It’s important to note that cloning creates a new VDI object: this is not the one we started with gone.
`,description:"",tags:null,title:"Snapshots",uri:"/new-docs/toolstack/features/snapshots/index.html"},{content:`Example suspend image layout:
+----------------------------+ | 1. Suspend image signature | +============================+ | 2.0 Xenops header | | 2.1 Xenops record | +============================+ | 3.0 Libxc header | | 3.1 Libxc record | +============================+ | 4.0 Qemu header | | 4.1 Qemu save record | +============================+ | 5.0 End_of_image footer | +----------------------------+ A suspend image is now constucted as a series of header-record pairs. The initial signature (1.) is used to determine whether we are dealing with the unstructured, “legacy” suspend image or the new, structured format.
Each header is two 64-bit integers: the first identifies the header type and the second is the length of the record that follows in bytes. The following types have been defined (the ones marked with a (*) have yet to be implemented):
* Xenops : Metadata for the suspend image * Libxc : The result of a xc_domain_save * Libxl* : Not implemented * Libxc_legacy : Marked as a libxc record saved using pre-Xen-4.5 * Qemu_trad : The qemu save file for the Qemu used in XenServer * Qemu_xen* : Not implemented * Demu* : Not implemented * End_of_image : A footer marker to denote the end of the suspend image Some of the above types do not have the notion of a length since they cannot be known upfront before saving and also are delegated to other layers of the stack on restoring. Specifically these are the memory image sections, libxc and libxl.
`,description:"",tags:null,title:"Suspend image framing format",uri:"/new-docs/xenopsd/design/suspend-image-framing-format/index.html"},{content:"",description:"",tags:null,title:"Tags",uri:"/new-docs/tags/index.html"},{content:`Some operations performed by Xenopsd are blocking, for example:
suspend/resume/migration attaching disks (where the SMAPI VDI.attach/activate calls can perform network I/O) We want to be able to
present the user with an idea of progress (perhaps via a “progress bar”) allow the user to cancel a blocked operation that is taking too long associate logging with the user/client-initiated actions that spawned them Principles all operations which may block (the vast majority) should be written in an asynchronous style i.e. the operations should immediately return a Task id all operations should guarantee to respond to a cancellation request in a bounded amount of time (30s) when cancelled, the system should always be left in a valid state clients are responsible for destroying Tasks when they are finished with the results Types A task has a state, which may be Pending, Completed or failed:
type async_result = unit type completion_t = { duration : float; result : async_result option } type state = | Pending of float | Completed of completion_t | Failed of Rpc.tWhen a task is Failed, we assocate it with a marshalled exception (a value of type Rpc.t). This exception must be one from the set defined in the Xenops_interface. To see how they are marshalled, see Xenops_server.
From the point of view of a client, a Task has the immutable type (which can be queried with a Task.stat):
type t = { id: id; dbg: string; ctime: float; state: state; subtasks: (string * state) list; debug_info: (string * string) list; }where
id is a unique (integer) id generated by Xenopsd. This is how a Task is represented to clients dbg is a client-provided debug key which will be used in log lines, allowing lines from the same Task to be associated together ctime is the creation time state is the current state (Pending/Completed/Failed) subtasks lists logical internal sub-operations for debugging debug_info includes miscellaneous key/value pairs used for debugging Internally, Xenopsd uses a mutable record type to track Task state. This is broadly similar to the interface type except
the state is mutable: this allows Tasks to complete the task contains a “do this now” thunk there is a “cancelling” boolean which is toggled to request a cancellation. there is a list of cancel callbacks there are some fields related to “cancel points” Persistence The Tasks are intended to represent activities associated with in-memory queues and threads. Therefore the active Tasks are kept in memory in a map, and will be lost over a process restart. This is desirable since we will also lose the queued items and the threads, so there is no need to resync on start.
Note that every operation must ensure that the state of the system is recoverable on restart by not leaving it in an invalid state. It is not necessary to either guarantee to complete or roll-back a Task. Tasks are not expected to be transactional.
Lifecycle of a Task All Tasks returned by API functions are created as part of the enqueue functions: queue_operation_*. Even operations which are performed internally are normally wrapped in Tasks by the function immediate_operation.
A queued operation will be processed by one of the queue worker threads. It will
set the thread-local debug key to the Task.dbg call task.Xenops_task.run, taking care to catch exceptions and update the task.Xenops_task.state unset the thread-local debug key generate an event on the Task to provoke clients to query the current state. Task implementations must update their progress as they work. For the common case of a compound operation like VM_start which is decomposed into multiple “micro-ops” (e.g. VM_create VM_build) there is a useful helper function perform_atomics which divides the progress ‘bar’ into sections, where each “micro-op” can have a different size (weight). A progress callback function is passed into each Xenopsd backend function so it can be updated with fine granulatiry. For example note the arguments to B.VM.save
Clients are expected to destroy Tasks they are responsible for creating. Xenopsd cannot do this on their behalf because it does not know if they have successfully queried the Task status/result.
When Xenopsd is a client of itself, it will take care to destroy the Task properly, for example see immediate_operation.
Cancellation The goal of cancellation is to unstick a blocked operation and to return the system to some valid state, not any valid state in particular. Xenopsd does not treat operations as transactions; when an operation is cancelled it may
fully complete (e.g. if it was about to do this anyway) fully abort (e.g. if it had made no progress) enter some other valid state (e.g. if it had gotten half way through) Xenopsd will never leave the system in an invalid state after cancellation.
Every Xenopsd operation should unblock and return the system to a valid state within a reasonable amount of time after a cancel request. This should be as quick as possible but up to 30s may be acceptable. Bear in mind that a human is probably impatiently watching a UI say “please wait” and which doesn’t have any notion of progress itself. Keep it quick!
Cancellation is triggered by TASK.cancel which calls cancel. This
sets the cancelling boolean calls all registered cancel callbacks Implementations respond to cancellation by
if running: periodically call check_cancelling if about to block: register a suitable cancel callback safely with with_cancel. Xenopsd’s libxc backend can block in 2 different ways, and therefore has 2 different types of cancel callback:
cancellable Xenstore watches cancellable subprocesses Xenstore watches are used for device hotplug and unplug. Xenopsd has to wait for the backend or for a udev script to do something. If that blocks then we need a way to cancel the watch. The easiest way to cancel a watch is to watch an additional path (a “cancel path”) and delete it, see cancellable_watch. The “cancel paths” are placed within the VM’s Xenstore directory to ensure that cleanup code which does xenstore-rm will automatically “cancel” all outstanding watches. Note that we trigger a cancel by deleting rather than creating, to avoid racing with delete and creating orphaned Xenstore entries.
Subprocesses are used for suspend/resume/migrate. Xenopsd hands file descriptors to libxenguest by running a subprocess and passing the fds to it. Xenopsd therefore gets the process id and can send it a signal to cancel it. See Cancellable_subprocess.run.
Testing with cancel points Cancellation is difficult to test, as it is completely asynchronous. Therefore Xenopsd has some built-in cancellation testing infrastructure known as “cancel points”. A “cancel point” is a point in the code where a Cancelled exception could be thrown, either by checking the cancelling boolean or as a side-effect of a cancel callback. The check_cancelling function increments a counter every time it passes one of these points, and this value is returned to clients in the Task.debug_info.
A test harness runs a series of operations. Each operation is first run all the way through to completion to discover the total number of cancel points. The operation is then re-run with a request to cancel at a particular point. The test then waits for the system to stabilise and verifies that it appears to be in a valid state.
Preventing Tasks leaking The client who creates a Task must destroy it when the Task is finished, and they have processed the result. What if a client like xapi is restarted while a Task is running?
We assume that, if xapi is talking to a xenopsd, then xapi completely owns it. Therefore xapi should destroy any completed tasks that it doesn’t recognise.
If a user wishes to manage VMs with xenopsd in parallel with xapi, the user should run a separate xenopsd.
`,description:"",tags:null,title:"Tasks",uri:"/new-docs/xenopsd/design/Tasks/index.html"},{content:`XenServer has supported passthrough for GPU devices since XenServer 6.0. Since the advent of NVIDIA’s vGPU-capable GRID K1/K2 cards it has been possible to carve up a GPU into smaller pieces yielding a more scalable solution to boosting graphics performance within virtual machines.
The K1 has four GK104 GPUs and the K2 two GK107 GPUs. Each of these will be exposed through Xapi so a host with a single K1 card will have access to four independent PGPUs.
Each of the GPUs can then be subdivided into vGPUs. For each type of PGPU, there are a few options of vGPU type which consume different amounts of the PGPU. For example, K1 and K2 cards can currently be configured in the following ways:
Note, this diagram is not to scale, the PGPU resource required by each vGPU type is as follows:
vGPU type PGPU kind vGPUs / PGPU k100 GK104 8 k140Q GK104 4 k200 GK107 8 k240Q GK107 4 k260Q GK107 2 Currently each physical GPU (PGPU) only supports homogeneous vGPU configurations but different configurations are supported on different PGPUs across a single K1/K2 card. This means that, for example, a host with a K1 card can run 64 VMs with k100 vGPUs (8 per PGPU).
XenServer’s vGPU architecture A new display type has been added to the device model:
@@ -4519,6 +4522,7 @@ static const QEMUOption qemu_options[] = /* Xen tree options: */ { "std-vga", 0, QEMU_OPTION_std_vga }, + { "vgpu", 0, QEMU_OPTION_vgpu }, { "videoram", HAS_ARG, QEMU_OPTION_videoram }, { "d", HAS_ARG, QEMU_OPTION_domid }, /* deprecated; for xend compatibility */ { "domid", HAS_ARG, QEMU_OPTION_domid }, With this in place, qemu can now be started using a new option that will enable it to communicate with a new display emulator, vgpu to expose the graphics device to the guest. The vgpu binary is responsible for handling the VGX-capable GPU and, once it has been successfully passed through, the in-guest drivers can be installed in the same way as when it detects new hardware.
The diagram below shows the relevant parts of the architecture for this project.
Relevant code In Xenopsd: Xenops_server_xen is where Xenopsd gets the vGPU information from the values passed from Xapi; In Xenopsd: Device.__start is where the vgpu process is started, if necessary, before Qemu. Xapi’s API and data model A lot of work has gone into the toolstack to handle the creation and management of VMs with vGPUs. We revised our data model, introducing a semantic link between VGPU and PGPU objects to help with utilisation tracking; we maintained the GPU_group concept as a pool-wide abstraction of PGPUs available for VMs; and we added VGPU_types which are configurations for VGPU objects.
Aside: The VGPU type in Xapi’s data model predates this feature and was synonymous with GPU-passthrough. A VGPU is simply a display device assigned to a VM which may be a vGPU (this feature) or a whole GPU (a VGPU of type passthrough).
VGPU_types can be enabled/disabled on a per-PGPU basis allowing for reservation of particular PGPUs for certain workloads. VGPUs are allocated on PGPUs within their GPU group in either a depth-first or breadth-first manner, which is configurable on a per-group basis.
VGPU_types are created by xapi at startup depending on the available hardware and config files present in dom0. They exist in the pool database, and a primary key is used to avoid duplication. In XenServer 6.x the tuple of (vendor_name, model_name) was used as the primary key, however this was not ideal as these values are subject to change. XenServer 7.0 switched to a new primary key generated from static metadata, falling back to the old method for backwards compatibility.
A VGPU_type will be garbage collected when there is no VGPU of that type and there is no hardware which supports that type. On VM import, all VGPUs and VGPU_types will be created if necessary - if this results in the creation of a new VGPU_type then the VM will not be usable until the required hardware and drivers are installed.
Relevant code In Xapi: Xapi_vgpu_type contains the type definitions and parsing logic for vGPUs; In Xapi: Xapi_pgpu_helpers defines the functions used to allocate vGPUs on PGPUs. Xapi <-> Xenopsd interface In XenServer 6.x, all VGPU config was added to the VM’s platform field at startup, and this information was used by xenopsd to start the display emulator. See the relevant code here.
In XenServer 7.0, to facilitate support of VGPU on Intel hardware in parallel with the existing NVIDIA support, VGPUs were made first-class objects in the xapi-xenopsd interface. The interface is described here.
VM startup On the pool master:
Assuming no WLB, all VM.start tasks pass through Xapi_vm_helpers.choose_host_for_vm_no_wlb. If the VM has a vGPU, the list of all hosts in the pool is split into a list of lists, where the first list is the most optimal in terms of the GPU group’s allocation mode and the PGPU availability on each host. Each list of hosts in turn is passed to Xapi_vm_placement.select_host, which checks storage, network and memory availability, until a suitable host is found. Once a host has been chosen, allocate_vm_to_host will set the VM.scheduled_to_be_resident_on and VGPU.scheduled_to_be_resident_on fields. The task is then ready to be forwarded to the host on which the VM will start:
If the VM has a VGPU, the startup task is wrapped in Xapi_gpumon.with_gpumon_stopped. This makes sure that the NVIDIA driver is not in use so can be loaded or unloaded from physical GPUs as required. The VM metadata, including VGPU metadata, is passed to xenopsd. The creation of the VGPU metadata is done by vgpus_of_vm. Note that at this point passthrough VGPUs are represented by the PCI device type, and metadata is generated by pcis_of_vm. As part of starting up the VM, xenopsd should report a VGPU event or a PCI event, which xapi will use to indicate that the xapi VGPU object can be marked as currently_attached. Usage To create a VGPU of a given type you can use vgpu-create:
$ xe vgpu-create vm-uuid=... gpu-group-uuid=... vgpu-type-uuid=...To see a list of VGPU types available for use on your XenServer, run the following command. Note: these will only be populated if you have installed the relevant NVIDIA RPMs and if there is hardware installed on that host supported each type. Using params=all will display more information such as the maximum number of heads supported by that VGPU type and which PGPUs have this type enabled and supported.
$ xe vgpu-type-list [params=all]To access the new and relevant parameters on a PGPU (i.e. supported_VGPU_types, enabled_VGPU_types, resident_VGPUs) you can use pgpu-param-get with param-name=supported-vgpu-types param-name=enabled-vgpu-types and param-name=resident-vgpus respectively. Or, alternatively, you can use the following command to list all the parameters for the PGPU. You can get the types supported or enabled for a given PGPU:
$ xe pgpu-list uuid=... params=all`,description:"",tags:null,title:"vGPU",uri:"/new-docs/toolstack/features/VGPU/index.html"},{content:`A Xenopsd client wishes to start a VM. They must first tell Xenopsd the VM configuration to use. A VM configuration is broken down into objects:
VM: A device-less Virtual Machine VBD: A virtual block device for a VM VIF: A virtual network interface for a VM PCI: A virtual PCI device for a VM Treating devices as first-class objects is convenient because we wish to expose operations on the devices such as hotplug, unplug, eject (for removable media), carrier manipulation (for network interfaces) etc.
The “add” functions in the Xenopsd interface cause Xenopsd to create the objects:
VM.add VBD.add VIF.add PCI.add In the case of xapi, there are a set of functions which convert between the XenAPI objects and the Xenopsd objects. The two interfaces are slightly different because they have different expected users:
the XenAPI has many clients which are updated on long release cycles. The main property needed is backwards compatibility, so that new release of xapi remain compatible with these older clients. Quite often we will chose to “grandfather in” some poorly designed interface simply because we wish to avoid imposing churn on 3rd parties. the Xenopsd API clients are all open-source and are part of the xapi-project. These clients can be updated as the API is changed. The main property needed is to keep the interface clean, so that it properly hides the complexity of dealing with Xen from other components. The Xenopsd “VM.add” function has code like this:
let add' x = debug "VM.add %s" (Jsonrpc.to_string (rpc_of_t x)); DB.write x.id x; let module B = (val get_backend () : S) in B.VM.add x; x.idThis function does 2 things:
it stores the VM configuration in the “database” it tells the “backend” that the VM exists The Xenopsd database is really a set of config files in the filesystem. All objects belonging to a VM (recall we only have VMs, VBDs, VIFs, PCIs and not stand-alone entities like disks) and are placed into a subdirectory named after the VM e.g.:
# ls /run/nonpersistent/xenopsd/xenlight/VM/7b719ce6-0b17-9733-e8ee-dbc1e6e7b701 config	vbd.xvda vbd.xvdb # cat /run/nonpersistent/xenopsd/xenlight/VM/7b719ce6-0b17-9733-e8ee-dbc1e6e7b701/config {"id": "7b719ce6-0b17-9733-e8ee-dbc1e6e7b701", "name": "fedora", ... }Xenopsd doesn’t have as persistent a notion of a VM as xapi, it is expected that all objects are deleted when the host is rebooted. However the objects should be persisted over a simple Xenopsd restart, which is why the objects are stored in the filesystem.
Aside: it would probably be more appropriate to store the metadata in Xenstore since this has the exact object lifetime we need. This will require a more performant Xenstore to realise.
Every running Xenopsd process is linked with a single backend. Currently backends exist for:
Xen via libxc, libxenguest and xenstore Xen via libxl, libxc and xenstore Xen via libvirt KVM by direct invocation of qemu Simulation for testing From here we shall assume the use of the “Xen via libxc, libxenguest and xenstore” (a.k.a. “Xenopsd classic”) backend.
The backend VM.add function checks whether the VM we have to manage already exists – and if it does then it ensures the Xenstore configuration is intact. This Xenstore configuration is important because at any time a client can query the state of a VM with VM.stat and this relies on certain Xenstore keys being present.
Once the VM metadata has been registered with Xenopsd, the client can call VM.start. Like all potentially-blocking Xenopsd APIs, this function returns a Task id. Please refer to the Task handling design for a general overview of how tasks are handled.
Clients can poll the state of a task by calling TASK.stat but most clients will prefer to use the event system instead. Please refer to the Event handling design for a general overview of how events are handled.
The event model is similar to the XenAPI: clients call a blocking UPDATES.get passing in a token which represents the point in time when the last UPDATES.get returned. The call blocks until some objects have changed state, and these object ids are returned (NB in the XenAPI the current object states are returned) The client must then call the relevant “stat” function, in this case TASK.stat
The client will be able to see the task make progress and use this to – for example – populate a progress bar in a UI. If the client needs to cancel the task then it can call the TASK.cancel; again see the Task handling design to understand how this is implemented.
When the Task has completed successfully, then calls to *.stat will show:
the power state is Paused exactly one valid Xen domain id all VBDs have active = plugged = true all VIFs have active = plugged = true all PCI devices have plugged = true at least one active console a valid start time valid “targets” for memory and vCPU Note: before a Task completes, calls to *.stat will show partial updates e.g. the power state may be Paused but none of the disks may have become plugged. UI clients must choose whether they are happy displaying this in-between state or whether they wish to hide it and pretend the whole operation has happened transactionally. If a particular client wishes to perform side-effects in response to Xenopsd state changes – for example to clean up an external resource when a VIF becomes unplugged – then it must be very careful to avoid responding to these in-between states. Generally it is safest to passively report these values without driving things directly from them. Think of them as status lights on the front panel of a PC: fine to look at but it’s not a good idea to wire them up to actuators which actually do things.
Note: the Xenopsd implementation guarantees that, if it is restarted at any point during the start operation, on restart the VM state shall be “fixed” by either (i) shutting down the VM; or (ii) ensuring the VM is intact and running.
In the case of xapi every Xenopsd Task id bound one-to-one with a XenAPI task by the function sync_with_task. The function update_task is called when xapi receives a notification that a Xenopsd Task has changed state, and updates the corresponding XenAPI task. Xapi launches exactly one thread per Xenopsd instance (“queue”) to monitor for background events via the function events_watch while each thread performing a XenAPI call waits for its specific Task to complete via the function event_wait.
It is the responsibility of the client to call TASK.destroy when the Task is nolonger needed. Xenopsd won’t destroy the task because it contains the success/failure result of the operation which is needed by the client.
What happens when a Xenopsd receives a VM.start request?
When Xenopsd receives the request it adds it to the appropriate per-VM queue via the function queue_operation. To understand this and other internal details of Xenopsd, consult the architecture description. The queue_operation_int function looks like this:
let queue_operation_int dbg id op = let task = Xenops_task.add tasks dbg (fun t -> perform op t; None) in Redirector.push id (op, task); taskThe “task” is a record containing Task metadata plus a “do it now” function which will be executed by a thread from the thread pool. The module Redirector takes care of:
pushing operations to the right queue ensuring at most one worker thread is working on a VM’s operations reducing the queue size by coalescing items together providing a diagnostics interface Once a thread from the worker pool becomes free, it will execute the “do it now” function. In the example above this is perform op t where op is VM_start vm and t is the Task. The function perform has fragments like this:
| VM_start id -> debug "VM.start %s" id; perform_atomics (atomics_of_operation op) t; VM_DB.signal idEach “operation” (e.g. VM_start vm) is decomposed into “micro-ops” by the function atomics_of_operation where the micro-ops are small building-block actions common to the higher-level operations. Each operation corresponds to a list of “micro-ops”, where there is no if/then/else. Some of the “micro-ops” may be a no-op depending on the VM configuration (for example a PV domain may not need a qemu). In the case of VM_start vm this decomposes into the sequence:
1. run the “VM_pre_start” scripts The VM_hook_script micro-op runs the corresponding “hook” scripts. The code is all in the Xenops_hooks module and looks for scripts in the hardcoded path /etc/xapi.d.
2. create a Xen domain The VM_create micro-op calls the VM.create function in the backend. In the classic Xenopsd backend the VM.create_exn function must
check if we’re creating a domain for a fresh VM or resuming an existing one: if it’s a resume then the domain configuration stored in the VmExtra database table must be used ask squeezed to create a memory “reservation” big enough to hold the VM memory. Unfortunately the domain cannot be created until the memory is free because domain create often fails in low-memory conditions. This means the “reservation” is associated with our “session” with squeezed; if Xenopsd crashes and restarts the reservation will be freed automatically. create the Domain via the libxc hypercall “transfer” the squeezed reservation to the domain such that squeezed will free the memory if the domain is destroyed later compute and set an initial balloon target depending on the amount of memory reserved (recall we ask for a range between dynamic_min and dynamic_max) apply the “suppress spurious page faults” workaround if requested set the “machine address size” “hotplug” the vCPUs. This operates a lot like memory ballooning – Xen creates lots of vCPUs and then the guest is asked to only use some of them. Every VM therefore starts with the “VCPUs_max” setting and co-operative hotplug is used to reduce the number. Note there is no enforcement mechanism: a VM which cheats and uses too many vCPUs would have to be caught by looking at the performance statistics. 3. build the domain On a Xen system a domain is created empty, and memory is actually allocated from the host in the “build” phase via functions in libxenguest. The VM.build_domain_exn function must
run pygrub (or eliloader) to extract the kernel and initrd, if necessary invoke the xenguest binary to interact with libxenguest. apply the cpuid configuration store the current domain configuration on disk – it’s important to know the difference between the configuration you started with and the configuration you would use after a reboot because some properties (such as maximum memory and vCPUs) as fixed on create. The xenguest binary was originally a separate binary for two reasons: (i) the libxenguest functions weren’t threadsafe since they used lots of global variables; and (ii) the libxenguest functions used to have a different, incompatible license, which prevent us linking. Both these problems have been resolved but we still shell out to the xenguest binary.
The xenguest binary has also evolved to configure more of the initial domain state. It also reads Xenstore and configures
the vCPU affinity the vCPU credit2 weight/cap parameters whether the NX bit is exposed whether the viridian CPUID leaf is exposed whether the system has PAE or not whether the system has ACPI or not whether the system has nested HVM or not whether the system has an HPET or not 4. mark each VBD as “active” VBDs and VIFs are said to be “active” when they are intended to be used by a particular VM, even if the backend/frontend connection hasn’t been established, or has been closed. If someone calls VBD.stat or VIF.stat then the result includes both “active” and “plugged”, where “plugged” is true if the frontend/backend connection is established. For example xapi will set VBD.currently_attached to “active || plugged”. The “active” flag is conceptually very similar to the traditional “online” flag (which is not documented in the upstream Xen tree as of Oct/2014 but really should be) except that on unplug, one would set the “online” key to “0” (false) first before initiating the hotunplug. By contrast the “active” flag is set to false after the unplug i.e. “set_active” calls bracket plug/unplug. If the “active” flag was set before the unplug attempt then as soon as the frontend/backend connection is removed clients would see the VBD as completely dissociated from the VM – this would be misleading because Xenopsd will not have had time to use the storage API to release locks on the disks. By doing all the cleanup before setting “active” to false, clients can be assured that the disks are now free to be reassigned.
5. handle non-persistent disks A non-persistent disk is one which is reset to a known-good state on every VM start. The VBD_epoch_begin is the signal to perform any necessary reset.
6. plug VBDs The VBD_plug micro-op will plug the VBD into the VM. Every VBD is plugged in a carefully-chosen order. Generally, plug order is important for all types of devices. For VBDs, we must work around the deficiency in the storage interface where a VDI, once attached read/only, cannot be attached read/write. Since it is legal to attach the same VDI with multiple VBDs, we must plug them in such that the read/write VBDs come first. From the guest’s point of view the order we plug them doesn’t matter because they are indexed by the Xenstore device id (e.g. 51712 = xvda).
The function VBD.plug will
call VDI.attach and VDI.activate in the storage API to make the devices ready (start the tapdisk processes etc) add the Xenstore frontend/backend directories containing the block device info add the extra xenstore keys returned by the VDI.attach call that are needed for SCSIid passthrough which is needed to support VSS write the VBD information to the Xenopsd database so that future calls to VBD.stat can be told about the associated disk (this is needed so clients like xapi can cope with CD insert/eject etc) if the qemu is going to be in a different domain to the storage, a frontend device in the qemu domain is created. The Xenstore keys are written by the functions Device.Vbd.add_async and Device.Vbd.add_wait. In a Linux domain (such as dom0) when the backend directory is created, the kernel creates a “backend device”. Creating any device will cause a kernel UEVENT to fire which is picked up by udev. The udev rules run a script whose only job is to stat(2) the device (from the “params” key in the backend) and write the major and minor number to Xenstore for blkback to pick up. (Aside: FreeBSD doesn’t do any of this, instead the FreeBSD kernel module simply opens the device in the “params” key). The script also writes the backend key “hotplug-status=connected”. We currently wait for this key to be written so that later calls to VBD.stat will return with “plugged=true”. If the call returns before this key is written then sometimes we receive an event, call VBD.stat and conclude erroneously that a spontaneous VBD unplug occurred.
7. mark each VIF as “active” This is for the same reason as VBDs are marked “active”.
8. plug VIFs Again, the order matters. Unlike VBDs, there is no read/write read/only constraint and the devices have unique indices (0, 1, 2, …) but Linux kernels have often (always?) ignored the actual index and instead relied on the order of results from the xenstore-ls listing. The order that xenstored returns the items happens to be the order the nodes were created so this means that (i) xenstored must continue to store directories as ordered lists rather than maps (which would be more efficient); and (ii) Xenopsd must make sure to plug the vifs in the same order. Note that relying on ethX device numbering has always been a bad idea but is still common. I bet if you change this lots of tests will suddenly start to fail!
The function VIF.plug_exn will
compute the port locking configuration required and write this to a well-known location in the filesystem where it can be read from the udev scripts. This really should be written to Xenstore instead, since this scheme doesn’t work with driver domains. add the Xenstore frontend/backend directories containing the network device info write the VIF information to the Xenopsd database so that future calls to VIF.stat can be told about the associated network if the qemu is going to be in a different domain to the storage, a frontend device in the qemu domain is created. Similarly to the VBD case, the function Device.Vif.add will write the Xenstore keys and wait for the “hotplug-status=connected” key. We do this because we cannot apply the port locking rules until the backend device has been created, and we cannot know the rules have been applied until after the udev script has written the key. If we didn’t wait for it then the VM might execute without all the port locking properly configured.
9. create the device model The VM_create_device_model micro-op will create a qemu device model if
the VM is HVM; or the VM uses a PV keyboard or mouse (since only qemu currently has backend support for these devices). The function VM.create_device_model_exn will
(if using a qemu stubdom) it will create and build the qemu domain compute the necessary qemu arguments and launch it. Note that qemu (aka the “device model”) is created after the VIFs and VBDs have been plugged but before the PCI devices have been plugged. Unfortunately qemu traditional infers the needed emulated hardware by inspecting the Xenstore VBD and VIF configuration and assuming that we want one emulated device per PV device, up to the natural limits of the emulated buses (i.e. there can be at most 4 IDE devices: {primary,secondary}{master,slave}). Not only does this create an ordering dependency that needn’t exist – and which impacts migration downtime – but it also completely ignores the plain fact that, on a Xen system, qemu can be in a different domain than the backend disk and network devices. This hack only works because we currently run everything in the same domain. There is an option (off by default) to list the emulated devices explicitly on the qemu command-line. If we switch to this by default then we ought to be able to start up qemu early, as soon as the domain has been created (qemu will need to know the domain id so it can map the I/O request ring).
10. plug PCI devices PCI devices are treated differently to VBDs and VIFs. If we are attaching the device to an HVM guest then instead of relying on the traditional Xenstore frontend/backend state machine we instead send RPCs to qemu requesting they be hotplugged. Note the domain is paused at this point, but qemu still supports PCI hotplug/unplug. The reasons why this doesn’t follow the standard Xenstore model are known only to the people who contributed this support to qemu. Again the order matters because it determines the position of the virtual device in the VM.
Note that Xenopsd doesn’t know anything about the PCI devices; concepts such as “GPU groups” belong to higher layers, such as xapi.
11. mark the domain as alive A design principle of Xenopsd is that it should tolerate failures such as being suddenly restarted. It guarantees to always leave the system in a valid state, in particular there should never be any “half-created VMs”. We achieve this for VM start by exploiting the mechanism which is necessary for reboot. When a VM wishes to reboot it causes the domain to exit (via SCHEDOP_shutdown) with a “reason code” of “reboot”. When Xenopsd sees this event VM_check_state operation is queued. This operation calls VM.get_domain_action_request to ask the question, “what needs to be done to make this VM happy now?”. The implementation checks the domain state for shutdown codes and also checks a special Xenopsd Xenstore key. When Xenopsd creates a Xen domain it sets this key to “reboot” (meaning “please reboot me if you see me”) and when Xenopsd finishes starting the VM it clears this key. This means that if Xenopsd crashes while starting a VM, the new Xenopsd will conclude that the VM needs to be rebooted and will clean up the current domain and create a fresh one.
12. unpause the domain A Xenopsd VM.start will always leave the domain paused, so strictly speaking this is a separate “operation” queued by the client (such as xapi) after the VM.start has completed. The function VM.unpause is reassuringly simple:
if di.Xenctrl.total_memory_pages = 0n then raise (Domain_not_built); Domain.unpause ~xc di.Xenctrl.domid; Opt.iter (fun stubdom_domid -> Domain.unpause ~xc stubdom_domid ) (get_stubdom ~xs di.Xenctrl.domid)`,description:"",tags:null,title:"Walkthrough: Starting a VM",uri:"/new-docs/xenopsd/walkthroughs/VM.start/index.html"},{content:`The Xapi Storage Migration (XSM) also known as “Storage Motion” allows
a running VM to be migrated within a pool, between different hosts and different storage simultaneously; a running VM to be migrated to another pool; a disk attached to a running VM to be moved to another SR. The following diagram shows how XSM works at a high level:
The slowest part of a storage migration is migrating the storage, since virtual disks can be very large. Xapi starts by taking a snapshot and copying that to the destination as a background task. Before the datapath connecting the VM to the disk is re-established, xapi tells tapdisk to start mirroring all writes to a remote tapdisk over NBD. From this point on all VM disk writes are written to both the old and the new disk. When the background snapshot copy is complete, xapi can migrate the VM memory across. Once the VM memory image has been received, the destination VM is complete and the original can be safely destroyed.
`,description:"",tags:null,title:"Xapi Storage Migration",uri:"/new-docs/toolstack/features/XSM/index.html"},{content:`The XAPI Toolstack:
Forms the control plane of both XenServer as well as xcp-ng, manages clusters of Xen hosts with shared storage and networking, has a full-featured API, used by clients such as XenCenter and Xen Orchestra. The XAPI Toolstack is an open-source project developed by the xapi project, a sub-project of the Linux Foundation Xen Project.
The source code is available on Github under the xapi-project. the main repository is xen-api.
This developer guide documents the internals of the Toolstack to help developers understand the code, fix bugs and add new features. It is a work-in-progress, with new documents added when ready and updated whenever needed.
`,description:"",tags:null,title:"XAPI Toolstack Developer Guide",uri:"/new-docs/index.html"}]